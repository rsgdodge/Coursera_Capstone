{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385dd4fd",
   "metadata": {},
   "source": [
    "# Capstone Project - A Restaurant In Boston\n",
    "<i>Applied Data Science Capstone, IBM/Coursera Data Science Professional Certification</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69433ddf",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [Data/Exploration](#data)\n",
    "* [Methodology](#methodology)\n",
    "* [Analysis](#analysis)\n",
    "* [Results and Discussion](#results)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442aaf24",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Introduction</span> <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500888cd",
   "metadata": {},
   "source": [
    "For my Capstone Project I'll be examining the question of where to put a restaurant in Boston, Massachusetts. The city of Boston has a rich culinary history, covering the gamut from New England seafood (\"say chowdah!\") to the Italian restaurants and bakeries of the North End to Chinatown to the ubiquitous Dunkin' Donuts, based in nearby Canton MA. With roughly 1700 restaurant venues (including 74 Dunkin' Donuts), there's no shortage of options. So where might one squeeze in another venue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e701ac0",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Data/Exploration</span> <a name=\"data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f924f",
   "metadata": {},
   "source": [
    "I didn't initially have any specific agenda in terms of how I'd solve the problem, so I pulled data from a host of different sources, demographics, venues, etc.; mapped it, sliced and diced it, looked for something to jump out, and eventually I found an angle. Before we get to that in the Methodology section, let's take a look at the data sources used...\n",
    "\n",
    "<h3>Venue info</h3>\n",
    "\n",
    "Foursquare EXPLORE endpoint\n",
    "<br>https://api.foursquare.com/v2/venues/explore\n",
    "\n",
    "<h3>Census Tract and Population info</h3>\n",
    "\n",
    "US Census Bureau geocoder, using benchmark 4 (Public_AR_Current) and vintage 410 (Census2010_Current)\n",
    "<br>https://geocoding.geo.census.gov/geocoder/\n",
    "\n",
    "<h3>Household Income info</h3>\n",
    "\n",
    "US Census Bureau American Community Survey 5-Year Data (2009-2019) (aka ACS5), report B19013_001E (Estimate Median household income in the past 12 months in 2019 inflation-adjusted dollars)\n",
    "<br>https://api.census.gov/data/2019/acs/acs5\n",
    "\n",
    "<h3>Boston Census Tracts</h3>\n",
    "\n",
    "Analyze Boston\n",
    "<br>https://data.boston.gov/dataset/census-2010-tracts1 via hub.arcgis.com\n",
    "\n",
    "<h3>Boston Census Tract to Neighborhood key</h3>\n",
    "\n",
    "Boston Planning & Development Agency; Neighborhood-Map-by-Census-Tract_Updated-March-2014-(1)-(2).pdf\n",
    "<br>http://www.bostonplans.org/ \n",
    "\n",
    "<h3>MBTA Rapid Transit, Commuter Rail, and Bus info</h3>\n",
    "\n",
    "Massachusetts Bureau of Geographic Information (MassGIS)\n",
    "<br>https://docs.digital.mass.gov/dataset/massgis-data-mbta-rapid-transit via hub.arcgis.com\n",
    "<br>https://docs.digital.mass.gov/dataset/massgis-data-trains via hub.arcgis.com\n",
    "<br>https://docs.digital.mass.gov/dataset/massgis-data-mbta-bus-routes-and-stops via hub.arcgis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad408959",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Methodology</span> <a name=\"methodology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9372a2",
   "metadata": {},
   "source": [
    "I started this project with a clean sheet and no preconceived notions about the direction I'd go. I could focus on a particular restaurant type, perhaps look for an under-represented cuisine (although that could raise questions on whether a given category is under-represented and an opportunity, or under-represented because the market has already spoken regarding that type of restaurant). Going a different direction, I could focus on a particular part of town which seemed ripe for growth.\n",
    "\n",
    "One decision I made early on was that as part of the project, I'll be breaking up the city into neighborhoods as we did in the NYC and Toronto exercises, but with a twist. In those exercises we used a list of points as centroids of city neighborhoods, and defined venues as being \"in\" those neighborhoods based on being within a certain radius of the centroid. I understand that this was done for the sake of simplicity and keeping us focused on the task at hand, but in reality a neighborhood is not typically a point or defined simply by proximity to a point. It is a bounded area, usually irregularly shaped. For this project I'll be dividing the map of Boston into real neighborhoods and placing the venues pulled from Foursquare into these irregularly shaped areas.\n",
    "\n",
    "The City of Boston Planning and Development Agency maintains all manner of maps and charts and so on regarding the defined neighborhoods of the city. Most critically for our purposes, they also maintain a \"decoder ring\" of what U.S. Census tracts correspond to what neighborhoods. Combining the city info with the census info will allow us to create our geographic plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0a0fc",
   "metadata": {},
   "source": [
    "### Datawork Begins: Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eecbb3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import descartes                                        # working with geometric objects, dependency of geopandas\n",
    "import fiona                                            # reading/writing GIS data, dependency of geopandas\n",
    "import folium                                           # map rendering\n",
    "from folium import plugins                              # folium plugins\n",
    "from folium.features import DivIcon                     # making fast markers\n",
    "from folium.plugins import HeatMap                      # making heatmaps\n",
    "from geojson import Point, MultiPolygon, Feature        # geography tools\n",
    "import geopandas as gpd                                 # working with geospatial data\n",
    "from geopy.geocoders import Nominatim                   # finding lat/long from address\n",
    "from io import StringIO                                 # creating in-memory file-like objects\n",
    "import itertools                                        # creating iterators for efficient looping\n",
    "import json                                             # handling JSON files\n",
    "import math                                             # mathematical functions defined by the C standard\n",
    "import matplotlib                                       # creating static, animated, and interactive visualizations\n",
    "import matplotlib.cm as cm                              # colormap handling\n",
    "import matplotlib.colors as colors                      # converting numbers or color arguments to RGB/A\n",
    "import matplotlib.image as mpimg                        # basic image loading/rescaling/isplay\n",
    "import matplotlib.lines as mlines                       # line editing\n",
    "import matplotlib.pyplot as plt                         # interactive plots\n",
    "import numpy as np                                      # multi-dimensional arrays and high-level mathematical functions\n",
    "import os                                               # using operating system dependent functionality\n",
    "import pandas as pd                                     # data analysis and manipulation\n",
    "from pandas import json_normalize                       # transforming JSON file into a pandas dataframe\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import pyproj                                           # map projections, dependency of geopandas\n",
    "from pyproj import CRS                                  # coordinate reference system info\n",
    "from pyproj import Proj                                 # coordinate transformation between CRS types\n",
    "from pyproj import Transformer                          # making repeated transformations faster\n",
    "from pywaffle import Waffle                             # waffle charts\n",
    "import requests                                         # handling requests such as when webscraping\n",
    "import rtree                                            # advanced spatial indexing features, dependency of geopandas\n",
    "from scipy.spatial.distance import cdist                # computing distance between points using Euclidean distance\n",
    "import shapefile                                        # working with shapefiles\n",
    "import shapely                                          # working with geo shapes, dependency of geopandas\n",
    "from shapely import geometry                            # shapely module\n",
    "from shapely.geometry import shape, Point, Polygon      # shapely module\n",
    "from shapely.geometry.multipolygon import MultiPolygon  # shapely module\n",
    "from shapely.ops import unary_union, transform          # shapely module\n",
    "import sklearn                                          # scikit-learn predictive data analysis\n",
    "from sklearn.cluster import KMeans                      # module for KMeans Clustering\n",
    "from textwrap import fill                               # wrap-around text in legends etc\n",
    "import turfpy                                           # polygon/map tools\n",
    "from turfpy import measurement                          # map measurements\n",
    "from turfpy.measurement import boolean_point_in_polygon # determining if a point is within a polygon\n",
    "import urllib.request                                   # pulling in URL data\n",
    "\n",
    "print('...LIBRARIES IMPORTED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392c339",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# disable false \"chained assignment\" warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "print('...WARNING DISABLED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8b7ef",
   "metadata": {},
   "source": [
    "### Setting Basic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff9c26",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hidden variables\n",
    "my_id = 'XXXXX'\n",
    "my_secret = 'XXXXX'\n",
    "my_token = 'XXXXX'\n",
    "census_key = 'XXXXX'\n",
    "MBTA_key = 'XXXXX'\n",
    "print('...HIDDEN VARIABLES SET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1096d81",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# setting some basic variables\n",
    "CLIENT_ID = my_id                          # your Foursquare ID\n",
    "CLIENT_SECRET = my_secret                  # your Foursquare Secret\n",
    "ACCESS_TOKEN = my_token                    # your Foursquare Access Token\n",
    "VERSION = '20210501'                       # Foursquare API version\n",
    "LIMIT = 1000                               # A default Foursquare API limit value\n",
    "RADIUS = 2000                              # search / explore radius in meters\n",
    "VENUE_SEARCH = '4af3a181f964a520fcee21e3'  # ID of venue to be searched for\n",
    "SEARCH_LOC = 'Boston, MA'                  # location key for SEARCH or EXPLORE endpoint (near=)\n",
    "SEARCH_CAT_ID = '4d4b7105d754a06374d81259' # category key for SEARCH or EXPLORE endpoint (categoryId=)\n",
    "SECTION = 'food'                           # section key for EXPLORE endpoint (section=)\n",
    "OFFSET1 = 100                              # offset key for EXPLORE endpoint (offset=)\n",
    "OFFSET2 = 200                              # offset key for EXPLORE endpoint (offset=)\n",
    "OFFSET3 = 300                              # offset key for EXPLORE endpoint (offset=)\n",
    "print('...FOURSQUARE API CREDENTIALS SET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d924fda",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# geolocator to get basic coordinates\n",
    "address = 'Boston, MA'\n",
    "geolocator = Nominatim(user_agent=\"rsgd_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print(\"Boston coordinates: latitude\",latitude,\"/ longitude\",longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22677cc2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# workaround for issues with local GEOS install\n",
    "shapely.speedups.disable()\n",
    "print(\"...SHAPELY SPEEDUPS DISABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bf33b",
   "metadata": {},
   "source": [
    "### File Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac46e69",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "# create shortened url for github files\n",
    "datafile_url = (\"https://raw.githubusercontent.com/rsgdodge/Coursera_Capstone/master/data\")\n",
    "\n",
    "# MBTA Commuter Rail Stations\n",
    "MBTA_Rail_Stop=gpd.read_file('https://opendata.arcgis.com/datasets/b741e3542a9f40898d49e776452efe63_0.geojson')\n",
    "# MBTA Commuter Rail Lines\n",
    "MBTA_Rail_Line=gpd.read_file('https://opendata.arcgis.com/datasets/b741e3542a9f40898d49e776452efe63_3.geojson')\n",
    "# MBTA Rapid Transit Stops\n",
    "MBTA_RT_Stop=gpd.read_file('https://opendata.arcgis.com/datasets/a9e4d01cbfae407fbf5afe67c5382fde_0.geojson')\n",
    "# MBTA Rapid Transit Lines\n",
    "MBTA_RT_Line=gpd.read_file('https://opendata.arcgis.com/datasets/a9e4d01cbfae407fbf5afe67c5382fde_3.geojson')\n",
    "# MBTA Bus Routes\n",
    "MBTA_Bus_Line=gpd.read_file('https://opendata.arcgis.com/datasets/cef8d0fe8b9d49fe9aa8f1b7524c6ac2_3.geojson')\n",
    "# download Boston Census Tracts geojson\n",
    "BosTract=gpd.read_file('https://opendata.arcgis.com/datasets/4a8eb4fb3be44ed5a1eec28551b9f3b2_0.geojson')\n",
    "\n",
    "# download github csv files as DataFrames; neighborhood to census key and FourSquare broad categories\n",
    "BPDA_Neighborhoods = pd.read_csv(f'{datafile_url}/BPDA_Neighborhoods.csv')\n",
    "Broad_Categories = pd.read_csv(f'{datafile_url}/Broad_Categories.csv')\n",
    "problem_hex = pd.read_csv(f'{datafile_url}/problem_hex.csv').astype('object')\n",
    "\n",
    "print(\"...JSON FILES IMPORTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de78a89",
   "metadata": {},
   "source": [
    "I took the census tract map of Boston from ArcGis and used GeoPandas \"dissolve\" to remove some of the internal boundaries of the geo data and aggregate the 180 census tracts into the defined neighborhoods, as well as dissolving all internal features to create a single city-sized object, we'll need that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa3579",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with imported JSONs\n",
    "# removing large water-only tract\n",
    "BosTractTemp = pd.DataFrame(BosTract, copy=True)\n",
    "BosTractTemp.drop(BosTractTemp[BosTractTemp['ALAND10'] < 1].index, inplace = True)\n",
    "BostonNoWater = gpd.GeoDataFrame(BosTractTemp, crs=4326)\n",
    "# dissolving interior borders to make an \"all of Boston\" geo\n",
    "BostonOutline = BostonNoWater.dissolve(aggfunc = 'sum')                                   # dissolving boundaries\n",
    "BostonOutline.drop(columns=['FID', 'OBJECTID'], inplace=True)                             # dropping columns\n",
    "BostonOutline32619 = BostonOutline.to_crs(epsg=32619)                                     # outline in alt CRS (m)\n",
    "# dissolving interior borders to make a \"Boston Neighborhoods\" geo\n",
    "BosHoods = pd.merge(BostonNoWater, BPDA_Neighborhoods, how='left', on='NAMELSAD10')\n",
    "BosHoods.drop(columns=['GEOID10_y', 'NAME10_y'], inplace=True)                            # drop duplicate columns\n",
    "BosHoods.rename(columns={\"2014 BPDA Neighborhood\":\"Neighborhood\",\"GEOID10_x\":\"GEOID10\",   # rename columns\n",
    "                           \"NAME10_x\":\"NAME10\"}, inplace=True)                            # rename columns  \n",
    "BostonHoods = BosHoods.dissolve(by='Neighborhood', aggfunc = 'sum')                       # dissolving boundaries\n",
    "BostonHoods.drop(columns=['FID', 'OBJECTID'], inplace=True)                               # dropping columns\n",
    "BostonHoods.reset_index(inplace=True, drop=False)                                         # putting index back in\n",
    "print(\"BosTract\",'Type:',type(BosTract),\"/ current CRS is:\",BosTract.crs,\"/ shape:\",BosTract.shape)\n",
    "print(\"BostonNoWater\",'Type:',type(BostonNoWater),\"/ current CRS is:\",BostonNoWater.crs,\"/ shape:\",BostonNoWater.shape)\n",
    "print(\"BostonOutline\",'Type:',type(BostonOutline),\"/ current CRS is:\",BostonOutline.crs,\"/ shape:\",BostonOutline.shape)\n",
    "print(\"BostonOutline32619\",'Type:',type(BostonOutline32619),\"/ current CRS is:\",BostonOutline32619.crs,\"/ shape:\",BostonOutline32619.shape)\n",
    "print(\"BostonHoods\",'Type:', type(BostonHoods), \"/ current CRS is:\",BostonHoods.crs,\"/ shape:\",BostonHoods.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc745ef7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# finding the centroid of the BostonOutline polygon\n",
    "# centroid in EPSG:4326 throws error, need to re-projecting from other\n",
    "BostonCentroid32619 = BostonOutline32619.geometry.centroid         # pulling centroid location\n",
    "BostonCentroid = BostonCentroid32619.to_crs(epsg=4326)             # converting from UTM style point back to lat/lon\n",
    "print(\" BostonCentroid (4326): \",BostonCentroid,\"\\n BostonCentroid32619: \",BostonCentroid32619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "BostonCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa3ad5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# finding coordinate min/max via Shapely .bounds\n",
    "Bos_minX = BostonOutline.bounds.minx # -71.191156\n",
    "Bos_minY = BostonOutline.bounds.miny # 42.227889\n",
    "Bos_maxX = BostonOutline.bounds.maxx # -70.920102\n",
    "Bos_maxY = BostonOutline.bounds.maxy # 42.404937\n",
    "Bos_minX32619 = BostonOutline32619.bounds.minx # 319335.919566\n",
    "Bos_minY32619 = BostonOutline32619.bounds.miny # 4.677277e+06 int is 4677277\n",
    "Bos_maxX32619 = BostonOutline32619.bounds.maxx # 341792.806255\n",
    "Bos_maxY32619 = BostonOutline32619.bounds.maxy # 4.696683e+06 int is 4696683\n",
    "# setting center points via centroid\n",
    "Bos_centerX = float(BostonCentroid.x) # -71.08097 longitude\n",
    "Bos_centerY = float(BostonCentroid.y) # 42.31953 latitude\n",
    "Bos_centerX32619 = float(BostonCentroid32619.x) # 328520.137058\n",
    "Bos_centerY32619 = float(BostonCentroid32619.y) # 44.687352e+06 int is 4687351\n",
    "print(\"...VARIABLES CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8134d3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating shapely polygons of city (no water)\n",
    "BostonPolygon = BostonOutline.geometry.unary_union\n",
    "BostonPolygon32619 = BostonOutline32619.geometry.unary_union\n",
    "print(\"...POLYGONS CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5766fec",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with MBTA data\n",
    "# rename cols\n",
    "MBTA_Rail_Stop.rename(columns={\"STATION\":\"Station\",\"LINE_BRNCH\":\"Line\"}, inplace=True)\n",
    "MBTA_Rail_Line.rename(columns={\"LINE_BRNCH\":\"Line\"}, inplace=True)\n",
    "MBTA_RT_Stop.rename(columns={\"STATION\":\"Station\",\"LINE\":\"Line\",\"ROUTE\":\"Route\"}, inplace=True)\n",
    "MBTA_RT_Line.rename(columns={\"LINE\":\"Line\",\"ROUTE\":\"Route\"}, inplace=True)\n",
    "MBTA_Bus_Line.rename(columns={\"ROUTE_DESC\":\"Route\",\"TRIP_HEADSIGN\":\"Headsign\"}, inplace=True)\n",
    "# change case\n",
    "MBTA_Rail_Stop['Station'] = MBTA_Rail_Stop['Station'].str.title()\n",
    "MBTA_Rail_Stop['Line'] = MBTA_Rail_Stop['Line'].str.title()\n",
    "MBTA_Rail_Line['Line'] = MBTA_Rail_Line['Line'].str.title()\n",
    "MBTA_RT_Stop['Line'] = MBTA_RT_Stop['Line'].str.title()\n",
    "MBTA_RT_Line['Line'] = MBTA_RT_Line['Line'].str.title()\n",
    "# create lon/lat from point geometry\n",
    "MBTA_Rail_Stop['lon'] = MBTA_Rail_Stop['geometry'].x\n",
    "MBTA_Rail_Stop['lat'] = MBTA_Rail_Stop['geometry'].y\n",
    "MBTA_RT_Stop['lon'] = MBTA_RT_Stop['geometry'].x\n",
    "MBTA_RT_Stop['lat'] = MBTA_RT_Stop['geometry'].y\n",
    "print(\"MBTA_Rail_Stop\",'Type:', type(MBTA_Rail_Stop), \"/ current CRS is:\",MBTA_Rail_Stop.crs)\n",
    "print(\"MBTA_Rail_Line\",'Type:', type(MBTA_Rail_Line), \"/ current CRS is:\",MBTA_Rail_Line.crs)\n",
    "print(\"MBTA_RT_Stop\",'Type:', type(MBTA_RT_Stop), \"/ current CRS is:\",MBTA_RT_Stop.crs)\n",
    "print(\"MBTA_RT_Line\",'Type:', type(MBTA_RT_Line), \"/ current CRS is:\",MBTA_RT_Line.crs)\n",
    "#print(\"MBTA_Bus_Stop\",'Type:', type(MBTA_Bus_Stop), \"/ current CRS is:\",MBTA_Bus_Stop.crs)\n",
    "print(\"MBTA_Bus_Line\",'Type:', type(MBTA_Bus_Line), \"/ current CRS is:\",MBTA_Bus_Line.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf4f2b",
   "metadata": {},
   "source": [
    "### Venue Data Work Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215a245",
   "metadata": {},
   "source": [
    "Regardless of how large you set the \"limit\" parameter in your API request, Foursquare's EXPLORE endpoint only returns up to the first 100* recommended venues. Fortunately these results are paginated, so you can use the OFFSET variable to re-run the request and get the next 100, and so on.\n",
    "\n",
    "<i>*Foursquare's documentation says EXPLORE will return up to 50, but it's returning 100, so...</i>\n",
    "\n",
    "The SEARCH endpoint will only retrieve 50 results no matter how high you set the limit, and there is no OFFSET parameter. The makeshift solution suggested by the internet is to repeatedly re-run the search with slightly shifted lat/lon coordinates to try to blanket the area, then remove the duplicates. That sounds extra messy, so we'll be sticking with the EXPLORE endpoint.\n",
    "\n",
    "EXPLORE can use a parameter to limit the type of venues returned, either \"categoryId=categorynumber\" or \"section=sectionname\". Category number requests automatically include all subcategories of that category number, so using category number 4d4b7105d754a06374d81259 (Food) will capture all the varieties of restaurant nested under that category. Another alternative is to use one of the section names, which are high level categories like food, drinks, shops, arts, outdoors, sights, and trending. We’ll just use \"section=food\".\n",
    "\n",
    "So we'll use EXPLORE with \"section=food\" and loop thru while incrementing the \"offset=\" parameter to fill out our universe. The next question is how many loops? One option would be to simply keep incrementing the offset until the request gave an error message, but that seems rather crude. When you put in an API call with an offset so high that there are no more results, rather than responding with a code 400 Bad Request, Foursquare returns a code 200 message stating that nothing was found and indicating the max number of venues meeting the query request. By running the coordinates of each of our 180 census tracts thru the EXPLORE with an excessively high offset, we can pull a list of the total number of venues within our specified radius of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481cf7d",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding Neighborhood data to BostonNoWater\n",
    "City_Geo = pd.merge(BostonNoWater, BPDA_Neighborhoods, how='left', on='NAMELSAD10')       # join on key\n",
    "City_Geo.drop(columns=['GEOID10_y', 'NAME10_y'], inplace=True)                            # drop duplicate columns\n",
    "City_Geo.rename(columns={\"2014 BPDA Neighborhood\":\"Neighborhood\",\"GEOID10_x\":\"GEOID10\",   # rename columns\n",
    "                           \"NAME10_x\":\"NAME10\"}, inplace=True)                            # rename columns\n",
    "City_Geo[\"Off0\"] = 0                                                                      # adding temp offset cols\n",
    "City_Geo[\"Off1\"] = 100                                                                    # adding temp offset cols\n",
    "City_Geo[\"Off2\"] = 200                                                                    # adding temp offset cols\n",
    "City_Geo[\"Off3\"] = 300                                                                    # adding temp offset cols\n",
    "City_Geo.reset_index(inplace=True, drop=True)\n",
    "City_Geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8ace7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating MaxVenues function\n",
    "def getMaxVenues(names, latitudes, longitudes):\n",
    "    max_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        url_max = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&section={}&radius={}&limit={}&offset=400'.format(\n",
    "            CLIENT_ID,CLIENT_SECRET,VERSION,lat,lng,SECTION,RADIUS,LIMIT)\n",
    "        maxresults = requests.get(url_max)\n",
    "        max_list.append([maxresults.json()['response'],name,lat,lng])\n",
    "    return(max_list)\n",
    "print(\"...'getMaxVenues' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffcd4c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# running MaxVenues function\n",
    "venuesMax = getMaxVenues(names=City_Geo['Neighborhood'],latitudes=City_Geo['INTPTLAT10'],\n",
    "                         longitudes=City_Geo['INTPTLON10'])\n",
    "print('...RETRIEVAL COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b159cf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with MaxVenue\n",
    "VMax1 = pd.DataFrame(venuesMax)\n",
    "VMax1.rename(columns={VMax1.columns[0]:\"Response\",VMax1.columns[1]:\"Neighborhood\",VMax1.columns[2]:\"Input Lat\",\n",
    "                      VMax1.columns[3]:\"Input Lon\"}, inplace = True)\n",
    "VMax2=json_normalize(VMax1['Response'])\n",
    "VMax3=VMax2.merge(VMax1, how='outer', left_index=True, right_index=True)\n",
    "VMax3.drop(columns=['groups','suggestedFilters.header','suggestedFilters.filters','suggestedBounds.ne.lat',\n",
    "                   'suggestedBounds.ne.lng','suggestedBounds.sw.lat','suggestedBounds.sw.lng','Response'], inplace=True)\n",
    "VMax3.rename(columns={\"headerLocation\":\"4SQ Loc\", \"headerFullLocation\":\"4SQ Full Loc\",\n",
    "                     \"headerLocationGranularity\":\"Loc Granularity\",\"query\":\"Query\",\n",
    "                     \"totalResults\":\"Venue Count\"},inplace=True)\n",
    "VMax=VMax3[['Neighborhood','Input Lat','Input Lon','Venue Count','4SQ Loc','4SQ Full Loc','Loc Granularity','Query']]\n",
    "VMax.sort_values(by=['4SQ Full Loc','Neighborhood'],ignore_index=True, inplace=True)\n",
    "print(\"maximum venue count at any input coord is\",VMax['Venue Count'].max())\n",
    "VMax.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4548e",
   "metadata": {},
   "source": [
    "Okay, the maximum number of venues in the radius of any of our 180 input locations is 250, so it looks like three loops should cover it. We'll use the defined function below and increment the offset at 0, 100, and 200, then combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abe5c9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create \"getNearbyVenuesX00\" function with Lat+Lng+Offset from input, remainder from basic variables at top\n",
    "def getNearbyVenuesX00(names, latitudes, longitudes, offset):\n",
    "    venues_list=[]\n",
    "    for name, lat, lng, offs in zip(names, latitudes, longitudes, offset):\n",
    "        # print(name)\n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&section={}&radius={}&limit={}&offset={}'.format(\n",
    "            CLIENT_ID,CLIENT_SECRET,VERSION,lat,lng,SECTION,RADIUS,LIMIT,offs)\n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(name,lat,lng,v['venue']['name'],v['venue']['id'],v['venue']['location']['lat'],\n",
    "                             v['venue']['location']['lng'],v['venue']['categories'][0]['name'],\n",
    "                             v['venue']['location']['distance']) for v in results])\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Input Neighborhood','Input Latitude','Input Longitude','Venue','Venue_ID',\n",
    "                             'Venue_Latitude','Venue_Longitude','Venue_Category','Venue_Distance']\n",
    "    return(nearby_venues)\n",
    "print(\"...'getNearbyVenuesX00' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85307f8d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Foursquare venues, first loop (offset zero)\n",
    "venues000 = getNearbyVenuesX00(names=City_Geo['Neighborhood'],latitudes=City_Geo['INTPTLAT10'],\n",
    "                            longitudes=City_Geo['INTPTLON10'],offset=City_Geo['Off0'])\n",
    "print('...FIRST LOOP RETRIEVAL COMPLETE')\n",
    "venues000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729b639",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Foursquare venues, second loop (offset 100)\n",
    "venues100 = getNearbyVenuesX00(names=City_Geo['Neighborhood'],latitudes=City_Geo['INTPTLAT10'],\n",
    "                            longitudes=City_Geo['INTPTLON10'],offset=City_Geo['Off1'])\n",
    "print('...SECOND LOOP RETRIEVAL COMPLETE')\n",
    "venues100.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b13a0f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Foursquare venues, third loop (offset 200)\n",
    "venues200 = getNearbyVenuesX00(names=City_Geo['Neighborhood'],latitudes=City_Geo['INTPTLAT10'],\n",
    "                            longitudes=City_Geo['INTPTLON10'],offset=City_Geo['Off2'])\n",
    "print('...THIRD LOOP RETRIEVAL COMPLETE')\n",
    "venues200.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065aa7d",
   "metadata": {},
   "source": [
    "Now time to combined the loop results. It's likely that many venues will be within the specified radius of more than one of our 180 census tract coordinates, so after combining we'll sort the results and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67ac3d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# assembling Venue results\n",
    "# removing temporary offset cols\n",
    "City_Geo.drop(columns=['Off0','Off1','Off2','Off3'], inplace=True)\n",
    "# merge each of the \"venues\" offset outputs into one dataframe\n",
    "venues_int = venues000.append(venues100, ignore_index=True)\n",
    "venues = venues_int.append(venues200, ignore_index=True)\n",
    "venues.reset_index(inplace=True, drop=True)\n",
    "# removing duplicate entries\n",
    "length_before = len(venues)\n",
    "venues.sort_values(by=['Venue_ID','Venue_Distance'],     # sorting the data\n",
    "                       ignore_index=True, inplace=True)  # adding last two arguements resets the index to the new order\n",
    "# the \"subset\" parameter specifies which columns to match on, otherwise it matches on all fields\n",
    "venues.drop_duplicates(subset=['Venue_ID','Venue_Latitude',\n",
    "                               'Venue_Longitude'], keep='first', ignore_index=True,inplace=True)\n",
    "length_after = len(venues)\n",
    "venues['VenLat'] = venues['Venue_Latitude'].astype('object')\n",
    "venues['VenLon'] = venues['Venue_Longitude'].astype('object')\n",
    "print(\"before\",length_before,\"/ after\",length_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b4f80",
   "metadata": {},
   "source": [
    "Twenty-three thousand before, twenty-three hundred after. Yup, there were a few duplicates.\n",
    "\n",
    "The US Census Bureau's geocoder API will accept a set of coordinates and send a response indicating what census tract those coordinates are in. We'll loop the coordinates of our venues thru the geocoder and assign each to a specific census tract. This particular chunk of code is a bit slow, as it's making twenty-three hundred requests one at a time. The Census Bureau geocoder page will accept bulk upload of CSV files of up to 10000 locations per file, but I decided it was more in the spirit of this assignment to run as much of it within Python as possible, so we'll go for the API. All things being equal, just running the cell and getting a beverage is still probably quicker than downloading the list of coordinates, uploading it to the geocoder webpage, downloading those results, pulling them back into the notebook, and fixing any formatting issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ba9ca",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create \"getCensusInfo\" function with Lat+Lng from input \"venues\"\n",
    "# determines what Census Tract each venue is located in\n",
    "# benchmark (4) = Public_AR_Current\n",
    "# vintage (410) = Census2010_Current\n",
    "\n",
    "def getCensusInfo(ven_ids, lats, lons):\n",
    "    census_list=[]\n",
    "    for ven_id, lat, lng in zip(ven_ids, lats, lons):\n",
    "        # print(ven_id)\n",
    "        url_batch_geocode = 'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={}&y={}&benchmark=4&vintage=410&format=json'.format(\n",
    "            lng,lat)\n",
    "        census_results = requests.get(url_batch_geocode).json()['result']['geographies']['Census Tracts']\n",
    "        census_list.append([(ven_id,lat,lng,v['POP100'],v['GEOID'],v['INTPTLAT'],v['NAME'],v['INTPTLON'],\n",
    "                             v['HU100']) for v in census_results])\n",
    "    census_info = pd.DataFrame([item for loc_list in census_list for item in loc_list])\n",
    "    census_info.columns = ['Venue_ID','Venue_Latitude','Venue_Longitude','Population','GEOID','INTPTLAT',\n",
    "                           'NAME','INTPTLON','Housing_Units']\n",
    "    return(census_info)\n",
    "print(\"...'getCensusInfo' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e652c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# running getCensusInfo\n",
    "census_response = getCensusInfo(ven_ids=venues['Venue_ID'],lats=venues['VenLat'],lons=venues['VenLon'])\n",
    "print('...CENSUS TRACT RETRIEVAL COMPLETE')\n",
    "census_response.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a2610",
   "metadata": {},
   "source": [
    "Now that we have our venues and we have census tract data to go with each, we can eliminate entries that were within our specified radius but are not actually in the City of Boston. Every venue location will now be tagged with a GEOID10 number from the Census Bureau. The format of the number is:\n",
    "\n",
    "SSCCCTTTTT with **S**tate: 2 digits, **C**ounty: 3 digits, **T**ract: 6 digits.\n",
    "\n",
    "So a GEOID10 of 25025020101 = State 25, County 025, Tract 020101 (201.01). Massachusetts is state 25, and Suffolk County is county 025, so if it doesn't start \"25025\" then it's not in Suffolk County MA. Next is filtering out the three communities in Suffolk County that aren't Boston; Chelsea (tracts 1601.01, 1602, 1603, 1604, 1605.01, 1605.02, 1606.01, 1606.02), Revere (tracts 1701, 1702, 1703, 1704, 1705.01, 1705.02, 1706.01, 1707.01, 1707.02, 1708), and Winthrop (tracts 1801.01, 1802, 1803.01, 1804, 1805). We'll pass our geocoded venue data thru a few filters, limiting to just those which start with '25025' to dump the entries from the counties of Middlesex, Essex, and Norfolk, then remove entries starting with '25025160', '25025170', and '25025180' to pull out Chelsea, Revere, and Winthrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdff076",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding Census data to Venue data\n",
    "venue_census = pd.merge(venues, census_response, how='left', on='Venue_ID')\n",
    "venue_census.drop(columns=['Venue_Latitude_y','Venue_Longitude_y','Input Latitude',              # drop dupe columns\n",
    "                           'Input Longitude'], inplace=True)\n",
    "venue_census.rename(columns={\"Venue_Latitude_x\":\"VenLatNum\", \"Venue_Longitude_x\":\"VenLonNum\",    # rename columns\n",
    "                             \"GEOID\":\"GEOID10\",\"NAME\":\"NAMELSAD10\"},inplace=True)\n",
    "venue_census=venue_census[['Venue_ID','Venue','Venue_Category','VenLat','VenLatNum','VenLon',    # re-ordering\n",
    "                           'VenLonNum','Population','Housing_Units','GEOID10','NAMELSAD10',\n",
    "                           'INTPTLAT','INTPTLON']]\n",
    "\n",
    "# removing non-Boston entries:\n",
    "vc_length_before = len(venue_census)\n",
    "venue_census = venue_census[venue_census.GEOID10.str.startswith(('25025'))]                      # drop non-Suffolk\n",
    "venue_census = venue_census[~venue_census.GEOID10.str.startswith(('25025160'))]                  # drop Chelsea\n",
    "venue_census = venue_census[~venue_census.GEOID10.str.startswith(('25025170'))]                  # drop Revere\n",
    "venue_census = venue_census[~venue_census.GEOID10.str.startswith(('25025180'))]                  # drop Winthrop\n",
    "venue_census.reset_index(inplace=True, drop=True)\n",
    "vc_length_after = len(venue_census)\n",
    "print(\"before\",vc_length_before,\"/ after\",vc_length_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957536e9",
   "metadata": {},
   "source": [
    "This is our final venue count. Next we'll take our cleaned up list of venues and use the associated census tracts to assign the venues to neighborhoods as defined by the Boston Planning & Development Agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4baf019",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding Neighborhood data to Venue/Census data\n",
    "ven_neigh = pd.merge(venue_census, City_Geo, how='inner', on='NAMELSAD10')                          # join on key\n",
    "ven_neigh.drop(columns=['GEOID10_y','INTPTLAT','INTPTLON'], inplace=True)                           # drop columns\n",
    "ven_neigh.rename(columns={\"Venue_Category\":\"Venue Category\",\"Venue_ID\":\"Venue ID\",                  # rename columns\n",
    "                          \"GEOID10_x\":\"GEOID10\",\"Housing_Units\":\"Housing Units\"}, inplace=True)\n",
    "ven_neigh.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# adding Broad Category to Venue/Census/Neighborhood data\n",
    "ven_combined = pd.merge(ven_neigh, Broad_Categories, how='left', on='Venue Category')               # join on key\n",
    "ven_combined=ven_combined[['Venue ID','Venue','Venue Category','Broad Category','Short Type',       # re-ordering columns\n",
    "                           'VenLat','VenLatNum','VenLon','VenLonNum','Neighborhood','Population',\n",
    "                           'Housing Units','GEOID10','NAMELSAD10','FID','OBJECTID','STATEFP10',\n",
    "                           'COUNTYFP10','TRACTCE10','NAME10','MTFCC10','FUNCSTAT10','ALAND10',\n",
    "                           'AWATER10','INTPTLAT10','INTPTLON10','Shape_STAr','Shape_STLe',\n",
    "                           'Shape__Area','Shape__Length','geometry']]\n",
    "ven_combined.reset_index(inplace=True, drop=True)\n",
    "ven_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac271cc",
   "metadata": {},
   "source": [
    "### Population Data Work Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be418438",
   "metadata": {},
   "source": [
    "Using a couple of different parts of the Census Bureau data resources, we'll pull in population, housing, and income info for each of the census tracts in the city. For various purposes, we’ll arrange this into DataFrames on a tract-by-tract basis, a neighborhood-by-neighborhood basis, and a whole-city basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc3f84",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create \"getCensusPop\" function with Lat+Lng from input, gets population and housing info for each census tract\n",
    "def getCensusPop(hoods, lats, lons):\n",
    "    census_list=[]\n",
    "    for hood, lat, lng in zip(hoods, lats, lons):\n",
    "        # print(hood)\n",
    "        url_batch_geocode = 'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={}&y={}&benchmark=4&vintage=410&format=json'.format(\n",
    "            lng,lat)\n",
    "        census_results = requests.get(url_batch_geocode).json()['result']['geographies']['Census Tracts']\n",
    "        census_list.append([(hood,lat,lng,v['POP100'],v['GEOID'],v['INTPTLAT'],v['NAME'],v['INTPTLON'],\n",
    "                             v['HU100']) for v in census_results])\n",
    "    census_info = pd.DataFrame([item for loc_list in census_list for item in loc_list])\n",
    "    census_info.columns = ['Neighborhood','Hood_Latitude','Hood_Longitude','Population','GEOID','INTPTLAT',\n",
    "                           'NAME','INTPTLON','Housing_Units']\n",
    "    return(census_info)\n",
    "print(\"...'getCensusInfo' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9ab66",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# running getCensusPop\n",
    "census_pop = getCensusPop(hoods=City_Geo['Neighborhood'],lats=City_Geo['INTPTLAT10'],lons=City_Geo['INTPTLON10'])\n",
    "print('...CENSUS POP RETRIEVAL COMPLETE')\n",
    "census_pop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b682fe",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# cleaning up census data\n",
    "census_pop=census_pop[['Neighborhood','Population','Housing_Units','GEOID','NAME','INTPTLAT',    # re-ordering columns\n",
    "                       'INTPTLON','Hood_Latitude', 'Hood_Longitude']]\n",
    "census_pop.rename(columns={\"GEOID\":\"GEOID10\", \"NAME\":\"NAMELSAD10\",                               # rename columns\n",
    "                           \"Housing_Units\":\"Housing Units\"}, inplace=True)\n",
    "census_pop.drop(columns=['Hood_Latitude', 'Hood_Longitude'], inplace=True)                       # drop dupe columns\n",
    "c_length_before = len(census_pop)\n",
    "census_pop.sort_values(by=['Neighborhood','GEOID10'],         # sorting the data\n",
    "                           ignore_index=True, inplace=True)   # adding last two args resets the index to the new order\n",
    "census_pop.drop_duplicates(subset=['GEOID10','NAMELSAD10'],   # removing duplicate entries\n",
    "                           keep='first', ignore_index=True,   # the \"subset\" parameter specifies which columns to \n",
    "                           inplace=True)                      # match on, otherwise it matches on all fields\n",
    "c_length_after = len(census_pop)\n",
    "print(\"before\",c_length_before,\"/ after\",c_length_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae510f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# get income info for each census tract in Suffolk County, variables in /data/2019/acs/acs5/groups/B19013\n",
    "url_medinc = 'https://api.census.gov/data/2019/acs/acs5?get=NAME,B19013_001E&for=tract:*&in=state:25&in=county:025&key={}'.format(census_key)\n",
    "results_medinc = requests.get(url_medinc).json()\n",
    "medinc=pd.DataFrame(results_medinc[1:], columns=results_medinc[0])                          # convert to dataframe\n",
    "medinc[['NAMELSAD10','County Name','State Name']]=medinc.NAME.str.split(\", \",expand=True)   # split col\n",
    "medinc['Median Household Income'] = medinc['B19013_001E'].astype('float64') \n",
    "medinc.rename(columns={\"state\":\"State\",\"county\":\"County\",\"tract\":\"TRACTCE10\"},inplace=True) # rename columns\n",
    "medinc=medinc[['NAMELSAD10','TRACTCE10','Median Household Income','State','State Name',     # re-ordering columns\n",
    "               'County','County Name']]\n",
    "medinc['Median Household Income'].replace(-666666666,np.nan,inplace=True)                   # replace wonky values\n",
    "medinc['Median Household Income'].replace(0,np.nan,inplace=True)                            # replace wonky values\n",
    "medinc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b7756",
   "metadata": {},
   "source": [
    "### CensusGeo - Tract\n",
    "Arranging the Census info based on tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6df952",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# turn \"groupby\" totals of venues into dataframe\n",
    "tract_venue_count = ven_combined.groupby('NAMELSAD10', dropna=False)[[\"Venue ID\"]].count().reset_index()\n",
    "tract_venue_count.rename(columns={\"Venue ID\":\"Venue Count\"}, inplace=True) # rename column\n",
    "tract_venue_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39664d03",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding Census Population data to City_Geo\n",
    "tractgeo_a = pd.merge(City_Geo, census_pop, on='NAMELSAD10')                                     # join on key\n",
    "tractgeo_a.drop(columns=['GEOID10_y','Neighborhood_y'], inplace=True)                            # drop dupe columns\n",
    "tractgeo_a.rename(columns={\"GEOID10_x\":\"GEOID10\",\"Neighborhood_x\":\"Neighborhood\"}, inplace=True) # rename column\n",
    "tractgeo_a.reset_index(inplace=True, drop=True)\n",
    "tStep1 = tractgeo_a.shape\n",
    "# adding venue count columns\n",
    "tractgeo_b = pd.merge(tractgeo_a, tract_venue_count, how='left', on='NAMELSAD10')                 # join on key\n",
    "tractgeo_b.reset_index(inplace=True, drop=True)\n",
    "tStep2 = tractgeo_b.shape\n",
    "# adding Census Income data\n",
    "tractgeo_c = pd.merge(tractgeo_b, medinc, how='left', on='NAMELSAD10')                           # join on key\n",
    "tractgeo_c.drop(columns=['TRACTCE10_y','INTPTLAT','INTPTLON','State','County'], inplace=True)    # drop dupe columns\n",
    "tractgeo_c.rename(columns={\"TRACTCE10_x\":\"TRACTCE10\"}, inplace=True)                             # rename columns\n",
    "tractgeo_c.reset_index(inplace=True, drop=True)\n",
    "tStep3 = tractgeo_c.shape\n",
    "# creating derived field: population density by tract\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "tractgeo_c['Population Density'] = (tractgeo_c['Population'] / (tractgeo_c['ALAND10'] / 2589988)).round(2)\n",
    "# creating derived field: venue density by tract\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "tractgeo_c['Venue Density'] = (tractgeo_c['Venue Count'] / (tractgeo_c['ALAND10'] / 2589988)).round(2)\n",
    "# creating derived field: population per venue by tract\n",
    "tractgeo_c['Pop per Venue'] = (tractgeo_c['Population'] / tractgeo_c['Venue Count']).round(2)\n",
    "# creating derived field: population per housing unit by tract\n",
    "tractgeo_c['Pop per HU'] = (tractgeo_c['Population'] / tractgeo_c['Housing Units']).round(2)\n",
    "# adding dummy 'pop per ven' value for tracts with 0 venues and large population, pop per ven = pop\n",
    "tractgeo_c.loc[(tractgeo_c['Pop per Venue'].isnull()) & (tractgeo_c['Population'] > 1000), 'Pop per Venue'] = tractgeo_c['Population']\n",
    "# addressing excessive values\n",
    "tractgeo_c['Pop per HU'].replace(np.inf,np.nan,inplace=True)\n",
    "tractgeo_c['Pop per HU'] = np.where((tractgeo_c['Pop per HU'] > 6), np.nan, tractgeo_c['Pop per HU'])\n",
    "tStep4 = tractgeo_c.shape\n",
    "# re-ordering columns\n",
    "tractgeo=tractgeo_c[['Neighborhood','NAMELSAD10','Population','Population Density','Housing Units',\n",
    "                     'Pop per HU','Venue Count','Venue Density','Pop per Venue','Median Household Income','FID',\n",
    "                     'OBJECTID','STATEFP10','State Name','COUNTYFP10','County Name','TRACTCE10','GEOID10','NAME10',\n",
    "                     'MTFCC10','FUNCSTAT10','ALAND10','AWATER10','INTPTLAT10','INTPTLON10','Shape_STAr',\n",
    "                     'Shape_STLe','Shape__Area','Shape__Length','geometry']]\n",
    "tractgeo.reset_index(inplace=True, drop=True)\n",
    "tractgeo.sort_values(by=['Neighborhood','NAMELSAD10'],ignore_index=True, inplace=True)\n",
    "tStep5 = tractgeo.shape\n",
    "print(\"shape changes:\",tStep1,\"/\",tStep2,\"/\",tStep3,\"/\",tStep4,\"/\",tStep5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175172f3",
   "metadata": {},
   "source": [
    "In the calculations above, I found there were a handful of tracts which had significant populations but no venues at all. As we'll see in a subsequent chart, letting these tracts fall to \"NaN\" (not a number) on the \"population per venue\" count would be a missed opportunity, so instead for any tract with population greater than 1000 and 0 venues, I assigned a \"population per venue\" figure equal to the population of the tract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a1671",
   "metadata": {},
   "source": [
    "### CensusGeo - Neighborhood\n",
    "Arranging the Census info aggregated to neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a247f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# groupby totals\n",
    "# turn \"groupby\" totals of venues into dataframe\n",
    "hood_venue_count = ven_combined.groupby('Neighborhood', dropna=False)[[\"Venue ID\"]].count().reset_index()\n",
    "hood_venue_count.rename(columns={\"Venue ID\":\"Venue Count (hood)\"}, inplace=True)                  # rename column\n",
    "# turn \"groupby\" totals of neighborhoods into dataframe\n",
    "hood_pop = tractgeo.groupby('Neighborhood', dropna=False)[[\"Population\"]].sum().reset_index()\n",
    "hood_pop.rename(columns={\"Population\":\"Population (hood)\"}, inplace=True)                         # rename column\n",
    "# turn \"groupby\" totals of neighborhood land area into dataframe\n",
    "hood_area = tractgeo.groupby('Neighborhood', dropna=False)[[\"ALAND10\",\"AWATER10\"]].sum().reset_index()\n",
    "hood_area.rename(columns={\"ALAND10\":\"ALAND10 (hood)\",\"AWATER10\":\"AWATER10 (hood)\"}, inplace=True) # rename column\n",
    "# turn \"groupby\" totals of neighborhood land area into dataframe\n",
    "hood_units = tractgeo.groupby('Neighborhood', dropna=False)[[\"Housing Units\"]].sum().reset_index()\n",
    "hood_units.rename(columns={\"Housing Units\":\"Housing Units (hood)\"}, inplace=True)                 # rename column\n",
    "print(\"hood_venue_count\",hood_venue_count.shape)\n",
    "print(\"hood_pop\",hood_pop.shape)\n",
    "print(\"hood_area\",hood_area.shape)\n",
    "print(\"hood_units\",hood_units.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23bc49",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding Neighborhood Venue Count data to hoodgeo\n",
    "hoodgeo_a = pd.merge(BostonHoods, hood_venue_count, how='left', on='Neighborhood')               # join on key\n",
    "hoodgeo_a.reset_index(inplace=True, drop=True)\n",
    "hStep1 = hoodgeo_a.shape\n",
    "# adding Neighborhood Population data to hoodgeo\n",
    "hoodgeo_b = pd.merge(hoodgeo_a, hood_pop, how='left', on='Neighborhood')                        # join on key\n",
    "hoodgeo_b.reset_index(inplace=True, drop=True)\n",
    "hStep2 = hoodgeo_b.shape\n",
    "# adding Neighborhood Area data to hoodgeo\n",
    "hoodgeo_c = pd.merge(hoodgeo_b, hood_area, how='left', on='Neighborhood')                       # join on key\n",
    "hoodgeo_c.drop(columns=['ALAND10 (hood)', 'AWATER10 (hood)'], inplace=True)                     # dropping columns\n",
    "hoodgeo_c.reset_index(inplace=True, drop=True)\n",
    "hStep3 = hoodgeo_c.shape\n",
    "# adding Neighborhood Area data to hoodgeo\n",
    "hoodgeo_d = pd.merge(hoodgeo_c, hood_units, how='left', on='Neighborhood')                        # join on key\n",
    "hoodgeo_d.rename(columns={'ALAND10':'ALAND10 (hood)','AWATER10':'AWATER10 (hood)','Shape_STAr':'Shape_STAr (hood)',\n",
    "                          'Shape_STLe':'Shape_STLe (hood)','Shape__Area':'Shape__Area (hood)',\n",
    "                          'Shape__Length':'Shape__Length (hood)'}, inplace=True)                  # rename columns\n",
    "hoodgeo_d.reset_index(inplace=True, drop=True)\n",
    "hStep4 = hoodgeo_d.shape\n",
    "# creating derived field: population density by hood\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "hoodgeo_d['Population Density (hood)'] = (hoodgeo_d['Population (hood)'] / \n",
    "                                           (hoodgeo_d['ALAND10 (hood)'] / 2589988)).round(2)\n",
    "# creating derived field: venue density by hood\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "hoodgeo_d['Venue Density (hood)'] = (hoodgeo_d['Venue Count (hood)'] / \n",
    "                                      (hoodgeo_d['ALAND10 (hood)'] / 2589988)).round(2)\n",
    "# creating derived field: population per venue unit by hood\n",
    "hoodgeo_d['Pop per Venue (hood)'] = (hoodgeo_d['Population (hood)'] / \n",
    "                                  hoodgeo_d['Venue Count (hood)']).round(2)\n",
    "# creating derived field: population per housing unit by hood\n",
    "hoodgeo_d['Pop per HU (hood)'] = (hoodgeo_d['Population (hood)'] / \n",
    "                                  hoodgeo_d['Housing Units (hood)']).round(2)\n",
    "# addressing excessive values\n",
    "hoodgeo_d['Population Density (hood)'].replace(np.inf,np.nan,inplace=True)\n",
    "hoodgeo_d['Venue Density (hood)'].replace(np.inf,np.nan,inplace=True)\n",
    "hoodgeo_d['Pop per HU (hood)'].replace(np.inf,np.nan,inplace=True)\n",
    "hoodgeo_d['Pop per HU (hood)'] = np.where((hoodgeo_d['Pop per HU (hood)'] > 6), np.nan, hoodgeo_d['Pop per HU (hood)'])\n",
    "hStep5 = hoodgeo_d.shape\n",
    "# re-ordering columns\n",
    "hoodgeo=hoodgeo_d[['Neighborhood','Population (hood)','Population Density (hood)','Housing Units (hood)',\n",
    "                   'Pop per HU (hood)','Venue Count (hood)','Venue Density (hood)','Pop per Venue (hood)',\n",
    "                   'ALAND10 (hood)','AWATER10 (hood)','Shape_STAr (hood)','Shape_STLe (hood)','Shape__Area (hood)',\n",
    "                   'Shape__Length (hood)','geometry']]\n",
    "hoodgeo.reset_index(inplace=True, drop=True)\n",
    "hoodgeo.sort_values(by=['Neighborhood'],ignore_index=True, inplace=True)\n",
    "hStep6 = hoodgeo.shape\n",
    "print(\"shape changes:\",hStep1,\"/\",hStep2,\"/\",hStep3,\"/\",hStep4,\"/\",hStep5,\"/\",hStep6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c502ff",
   "metadata": {},
   "source": [
    "### CensusGeo - City\n",
    "Arranging the Census info aggregated to the entire city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edeb88e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with city-level data\n",
    "citygeo_a = BostonOutline\n",
    "citygeo_a['Population (city)'] = tractgeo['Population'].sum()\n",
    "citygeo_a['Venue Count (city)'] = ven_combined['Venue ID'].count()\n",
    "citygeo_a['Housing Units (city)'] = tractgeo['Housing Units'].sum()\n",
    "citygeo_a.rename(columns={\"ALAND10\":\"ALAND10 (city)\",\"AWATER10\":\"AWATER10 (city)\"}, inplace=True) # rename column\n",
    "# creating derived field: population density for city\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "citygeo_a['Population Density (city)'] = (citygeo_a['Population (city)'] / \n",
    "                                          (citygeo_a['ALAND10 (city)'] / 2589988)).round(2)\n",
    "# creating derived field: venue density for city\n",
    "# ALAND10 value is square meters, divide by 2,589,988 to convert to square miles \n",
    "citygeo_a['Venue Density (city)'] = (citygeo_a['Venue Count (city)'] / \n",
    "                                     (citygeo_a['ALAND10 (city)'] / 2589988)).round(2)\n",
    "# creating derived field: population per venue for city\n",
    "citygeo_a['Pop per Venue (city)'] = (citygeo_a['Population (city)'] / \n",
    "                                     citygeo_a['Venue Count (city)']).round(2)\n",
    "# creating derived field: population per housing unit for city\n",
    "citygeo_a['Pop per HU (city)'] = (citygeo_a['Population (city)'] / \n",
    "                                  citygeo_a['Housing Units (city)']).round(2)\n",
    "citygeo=citygeo_a[['Population (city)','Population Density (city)','Housing Units (city)','Pop per HU (city)',\n",
    "                   'Venue Count (city)','Venue Density (city)','Pop per Venue (city)','ALAND10 (city)',\n",
    "                   'AWATER10 (city)','Shape_STAr','Shape_STLe','Shape__Area','Shape__Length','geometry']]\n",
    "citygeo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086fd0e",
   "metadata": {},
   "source": [
    "### Merge to Venue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ca893",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding all Census data back into venue list\n",
    "# adding tract data in\n",
    "venue_master_a = pd.merge(ven_combined, tractgeo, how='left', on='NAMELSAD10')                       # join on key\n",
    "vStep1 = venue_master_a.shape\n",
    "venue_master_a.rename(columns={'Neighborhood_x':'Neighborhood','Population_x':'Population',          # rename columns\n",
    "                              'Housing Units_x':'Housing Units','GEOID10_x':'GEOID10',\n",
    "                              'FID_x':'FID','OBJECTID_x':'OBJECTID','STATEFP10_x':'STATEFP10',\n",
    "                              'COUNTYFP10_x':'COUNTYFP10','TRACTCE10_x':'TRACTCE10','NAME10_x':'NAME10',\n",
    "                              'MTFCC10_x':'MTFCC10','FUNCSTAT10_x':'FUNCSTAT10','ALAND10_x':'ALAND10',\n",
    "                              'AWATER10_x':'AWATER10','INTPTLAT10_x':'INTPTLAT10',\n",
    "                              'INTPTLON10_x':'INTPTLON10'}, inplace=True)\n",
    "venue_master_a.drop(columns=['Neighborhood_y','Population_y','Housing Units_y','FID_y','OBJECTID_y', # drop dupe columns\n",
    "                             'STATEFP10_y','COUNTYFP10_y','TRACTCE10_y','GEOID10_y','NAME10_y',\n",
    "                             'MTFCC10_y','FUNCSTAT10_y','ALAND10_y','AWATER10_y','INTPTLAT10_y',\n",
    "                             'INTPTLON10_y','Shape_STAr_y','Shape_STLe_y','Shape__Area_y',\n",
    "                             'Shape__Length_y','Shape_STAr_x','Shape_STLe_x','Shape__Area_x',\n",
    "                             'Shape__Length_x','geometry_y','geometry_x'], inplace=True)\n",
    "venue_master_a.reset_index(inplace=True, drop=True)\n",
    "vStep2 = venue_master_a.shape\n",
    "# adding neighborhood data in\n",
    "venue_master_b = pd.merge(venue_master_a, hoodgeo, how='left', on='Neighborhood')                    # join on key\n",
    "venue_master_b.drop(columns=['geometry'], inplace=True)                                              # drop column\n",
    "venue_master_b.reset_index(inplace=True, drop=True)\n",
    "vStep3 = venue_master_b.shape\n",
    "venue_master_c = venue_master_b\n",
    "# adding citywide total cols\n",
    "venue_master_c['Population (city)'] = citygeo['Population (city)'].sum()\n",
    "venue_master_c['Venue Count (city)'] = citygeo['Venue Count (city)'].sum()\n",
    "venue_master_c['Housing Units (city)'] = citygeo['Housing Units (city)'].sum()\n",
    "venue_master_c['ALAND10 (city)'] = citygeo['ALAND10 (city)'].sum()\n",
    "venue_master_c['AWATER10 (city)'] = citygeo['AWATER10 (city)'].sum()\n",
    "venue_master_c['Population Density (city)'] = citygeo['Population Density (city)'].sum()\n",
    "venue_master_c['Venue Density (city)'] = citygeo['Venue Density (city)'].sum()\n",
    "venue_master_c['Pop per Venue (city)'] = citygeo['Pop per Venue (city)'].sum()\n",
    "venue_master_c['Pop per HU (city)'] = citygeo['Pop per HU (city)'].sum()\n",
    "venue_master_c.reset_index(inplace=True, drop=True)\n",
    "vStep4 = venue_master_c.shape\n",
    "venue_master=venue_master_b[['Venue ID','Venue','Venue Category','Broad Category','Short Type',      # ordering columns\n",
    "                             'VenLat','VenLatNum','VenLon','VenLonNum','Population',\n",
    "                             'Population Density','Housing Units','Pop per HU','Venue Count',\n",
    "                             'Venue Density','Pop per Venue','Median Household Income',\n",
    "                             'GEOID10','NAMELSAD10','FID','OBJECTID','STATEFP10','State Name',\n",
    "                             'COUNTYFP10','County Name','TRACTCE10','NAME10','MTFCC10','FUNCSTAT10',\n",
    "                             'ALAND10','AWATER10','INTPTLAT10','INTPTLON10','Neighborhood',\n",
    "                             'Population (hood)','Population Density (hood)','Housing Units (hood)',\n",
    "                             'Pop per HU (hood)','Venue Count (hood)','Venue Density (hood)',\n",
    "                             'Pop per Venue (hood)','ALAND10 (hood)','AWATER10 (hood)','Population (city)',\n",
    "                             'Population Density (city)','Housing Units (city)','Pop per HU (city)',\n",
    "                             'Venue Count (city)','Venue Density (city)','Pop per Venue (city)',\n",
    "                             'ALAND10 (city)','AWATER10 (city)']]\n",
    "venue_master.reset_index(inplace=True, drop=True)\n",
    "vStep5 = venue_master.shape\n",
    "print(\"shape changes:\",vStep1,\"/\",vStep2,\"/\",vStep3,\"/\",vStep4,\"/\",vStep5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98665eb2",
   "metadata": {},
   "source": [
    "### <span style='color:white;background:red;font-family:Helvetica'>&nbsp;Looking At The Data&nbsp;</span>\n",
    "Now we start looking at these piles of data we've accumulated and arranged, getting a sense of the shape of the city and it's neighborhoods. We'll start with a list of the neighborhoods with population, housing units, and total number of venues in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b4c6d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# groupby of high level neighborhood details\n",
    "tractgeo.groupby('Neighborhood', dropna=False)[[\"Population\", \"Housing Units\",\n",
    "                                                \"Venue Count\"]].sum().style.format('{0:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4120cc81",
   "metadata": {},
   "source": [
    "No, I don't know where those 535 people on the Harbor Islands are living, given the absence of housing units. The Census Bureau defines a housing unit as \"*A house, an apartment, a mobile home or trailer, a group of rooms, or a single room occupied as separate living quarters, or if vacant, intended for occupancy as separate living quarters. Separate living quarters are those in which the occupants live separately from any other individuals in the building and which have direct access from outside the building or through a common hall. For vacant units, the criteria of separateness and direct access are applied to the intended occupants whenever possible.*\" So perhaps they sleep on boats...\n",
    "\n",
    "Next we'll crunch some numbers on the categories of venues we pulled from Foursquare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_master.groupby('Venue Category', dropna=False)[\"Venue ID\"].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ad6c4",
   "metadata": {},
   "source": [
    "As we see above, Foursquare lists 90 categories for food venues in Boston. To try to flatten that down to something more manageable, I've introduced a higher level categorization called \"Broad Category\". The broad category groupings I've made are listed in the table below. One could quibble about what should go into what category, but this is my working set. Part of my motivation is that Foursquare allows varying degrees of specificity in categorization, resulting in 34 venues just being categorized as \"Asian\", 80 more specifically labeled as \"Chinese\", and then some labeled even more specifically as \"Szechuan\". Likewise there are \"Japanese\", then also \"Sushi\", \"Udon\", etc. The uneven degrees of specificity / granularity prove bothersome when trying to get accurate aggregate views; broadly speaking, East Asian restaurants are one the most common types in Boston, but split out into a dozen subgroups buries them in any ranking list.\n",
    "\n",
    "Another issue is that a rather unhelpful number of entries in Foursquare's data have generic categorizations like \"Food\" or \"Restaurant\". There are also a decent number of \"Café\" entries, which traditionally might mean \"Coffeehouse\", but upon closer inspection the label has been applied to everything from coffee shops to greasy spoon diners. I've lumped all these together in as \"General\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db662956",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# turn \"groupby\" totals of broad categories into dataframe\n",
    "ven_broad_cat = venue_master.groupby('Broad Category', dropna=False)[[\"Venue ID\"]].count()\n",
    "ven_broad_cat = ven_broad_cat.sort_values(by=['Venue ID'], ascending=False).reset_index()\n",
    "ven_broad_cat.rename(columns={\"Venue ID\":\"Venues\"}, inplace=True) # rename column\n",
    "\n",
    "# making Venue Cat to Broad Cat table\n",
    "cat_cat_a = venue_master[['Broad Category','Venue Category']]\n",
    "cat_cat_a.drop_duplicates(subset=['Broad Category','Venue Category'], keep='first', ignore_index=True,inplace=True)\n",
    "cat_cat_b = cat_cat_a.pivot_table(index=['Broad Category'],values='Venue Category',aggfunc=lambda x: ', '.join(x))\n",
    "cat_cat_b.reset_index(inplace=True, drop=False)\n",
    "cat_cat_c = pd.merge(cat_cat_b, ven_broad_cat, how='left', on='Broad Category')\n",
    "cat_cat_c = cat_cat_c[['Broad Category','Venues','Venue Category']]\n",
    "cat_styler = cat_cat_c.style.set_properties(subset=['Venue Category'],**{'text-align': 'left'})\\\n",
    ".set_properties(subset=['Broad Category'],**{'text-align': 'left', 'width': '130px'})\n",
    "cat_styler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "display(cat_styler.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d686fb",
   "metadata": {},
   "source": [
    "If you prefer a more colorful look at the distribution of broad categories, here's a waffle chart instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a79e53",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# waffle chart of categories\n",
    "# code below will pass a colormap to waffle that pywaffle won't take natively\n",
    "# n_categories=ven_broad_cat.shape[0]\n",
    "# colors=[plt.cm.gist_ncar(i/float(n_categories)) for i in range(n_categories)]\n",
    "fig=plt.figure(FigureClass=Waffle,\n",
    "               values=ven_broad_cat['Venues'], # / 5, # adding number makes each square worth x number of entries\n",
    "               rows=25,                               # can specify rows, cols, both, or neither\n",
    "               #columns=70,                           # can specify rows, cols, both, or neither\n",
    "               block_arranging_style='normal',      # options are 'normal', 'snake', 'new-line'\n",
    "               vertical=False,                        # orientation of graph\n",
    "               cmap_name=\"tab20\",                     # use either \"cmap_name\" or \"colors\"\n",
    "               #colors=colors,                        # use either \"cmap_name\" or \"colors\"\n",
    "               facecolor='#EEEEEE',                   # background color\n",
    "               #icon_size=20,icon_legend=True,        # when using Icons rather than squares\n",
    "               labels=[l for l in ven_broad_cat['Broad Category']],\n",
    "               #labels=[fill(l, 10) for l in ven_broad_cat['Broad Category']],\n",
    "               #labels=[\"{0} ({1})\".format(n[0], n[1]) for n in ven_broad_cat[['Broad Category', 'Venues']].itertuples()],\n",
    "               legend={'loc': 'upper left', 'bbox_to_anchor': (0,0), 'ncol': 11, 'framealpha': 0},\n",
    "               #legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n",
    "               title={'label':'Broad Categories of Boston Venues','loc':'left','fontdict':{'fontsize': 20}},\n",
    "               tight=True,\n",
    "               figsize=(20,16))                       # width, height, supposedly in inches\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fbcaf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create version DF verison of hoodgeo GDF for charts\n",
    "hoodgeo_temp = pd.DataFrame(hoodgeo)\n",
    "hoodgeo_temp.sort_values(by=['Pop per Venue (hood)','Neighborhood'],ignore_index=True, inplace=True)\n",
    "hoodgeo_temp.reset_index(inplace=True, drop=True)\n",
    "TX = float(citygeo['Pop per Venue (city)'])\n",
    "for i in range(35):\n",
    "    hoodgeo_temp['padding'] = ['3' if x < TX else '-39' for x in hoodgeo_temp['Pop per Venue (hood)']]\n",
    "hoodgeo_temp['padding'] = hoodgeo_temp.padding.astype(float)\n",
    "hoodgeo_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e094b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_value_labels(ax, spacing=5):     # Add labels to the end of each bar in a bar chart\n",
    "# Arguments:\n",
    "#   ax (matplotlib.axes.Axes): The matplotlib object containing the axes of the plot to annotate\n",
    "#   spacing (int): The distance between the labels and the bars\n",
    "    for rect in rects:                   # Get X and Y placement of label from rect\n",
    "        x_value = rect.get_width()\n",
    "        y_value = rect.get_y() + rect.get_height() / 2\n",
    "        space = 5                        # Number of points between bar and label\n",
    "        ha = 'left'                      # Vertical alignment for positive values\n",
    "#        if x_value < 0:                  # If value of bar is negative: Place label left of bar\n",
    "        if x_value < citygeo['Pop per Venue (city)'].loc[0]:\n",
    "            space *= -1                  # Invert space to place label to the left\n",
    "            ha = 'right'                 # Horizontally align label at right\n",
    "        label = \"{:.2f}\".format(x_value) # Use X value as label and format number with one decimal place\n",
    "        plt.annotate(                    # Create annotation\n",
    "            label,                       # Use `label` as label\n",
    "            (x_value, y_value),          # Place label at end of the bar\n",
    "            xytext=(space, 0),           # Horizontally shift label by `space`\n",
    "            textcoords=\"offset points\",  # Interpret `xytext` as offset in points\n",
    "            va='center',                 # Vertically center label\n",
    "            ha=ha)                       # Horizontally align label differently for positive and negative values\n",
    "print(\"...FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc769bf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Neighborhood by Population per Venue conventional bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "hbars = ax.barh(hoodgeo_temp['Neighborhood'], hoodgeo_temp['Pop per Venue (hood)'], height=0.8, align='center')\n",
    "ax.set_yticks(hoodgeo_temp['Neighborhood'])\n",
    "ax.set_yticklabels(hoodgeo_temp['Neighborhood'])\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_title('Neighborhoods by Population per Venue')\n",
    "ax.bar_label(hbars, padding=3, fmt='%.2f')\n",
    "# ax.invert_yaxis()  # labels read top-to-bottom\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e40c8a",
   "metadata": {},
   "source": [
    "This is the where the metaphorical lightbulb went on for me on how to approach this \"where to put a restaurant\" problem. As you'll see in subsequent maps, the Census Bureau data supplied land area and population for each tract, from which we can calculate population density. Add in the Foursquare data and we can also calculate venue density. Looking at areas of the city with higher population density and lower venue density got me thinking of how to tie those two stats together, and what I came up with was Population per Venue, the population of a tract or neighborhood divided by the number venues in a tract or neighborhood. Where in the city are there way more people than venues? I took the chart above and re-worked it to show the bars as being above and below the city-wide mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e7b2e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Neighborhood by Population per Venue diverging bar chart\n",
    "fig, axp = plt.subplots(figsize=(14, 10))\n",
    "plt.hlines(y=hoodgeo_temp['Neighborhood'],\n",
    "           xmin=citygeo['Pop per Venue (city)'],                           # auto-set to citywide average\n",
    "           xmax=hoodgeo_temp['Pop per Venue (hood)'],\n",
    "           colors='red', alpha=1, linewidth=18)                            # Plotting the horizontal lines\n",
    "rects = ax.patches\n",
    "plt.axvline(citygeo['Pop per Venue (city)'].loc[0], color='red', linewidth=0.8)\n",
    "add_value_labels('Pop per Venue (hood)',1)\n",
    "plt.gca().set(ylabel=None, xlabel='Population')                            # Setting the labels of x-axis and y-axis\n",
    "plt.title('Neighborhoods by Population per Venue', fontdict={'size': 20})  # Title of Bar Chart\n",
    "plt.grid(linestyle='--', alpha=0.5)                                        # Optional grid layout\n",
    "plt.show()                                                                 # Displaying the Diverging Bar Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100398c5",
   "metadata": {},
   "source": [
    "The red dividing line in the middle represents the city-wide average for people per venue, 370.04. Those in the bottom half have less people per venue, those in the top half have more people per venue. One thing which stands out immediately is that the neighborhoods in the southwest of the city (Mattapan, Hyde Park, Roxbury, Dorchester, West Roxbury) have by far the highest Population per Venue numbers, or to put it another way, far fewer venues per capita, meaning this is the area of the city is under-served in terms of restaurants and should be where we focus for placing a new venue. But first, some maps..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590acbb",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c26ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# converting main DataFrames to GeoJSON\n",
    "tractgeo['Population'].replace(0,np.nan,inplace=True)\n",
    "tractgeoJSON = tractgeo.to_json()                     # convert dataframe to json\n",
    "print(\"...JSON (tractgeo) created\")\n",
    "hoodgeo['Population (hood)'].replace(0,np.nan,inplace=True)\n",
    "hoodgeoJSON = hoodgeo.to_json()                       # convert dataframe to json\n",
    "print(\"...JSON (hoodgeo) created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9d8c8",
   "metadata": {},
   "source": [
    "It might be useful to select a location that isn't already surrounded by existing venues, so we'll use GeoPandas to add a  buffer around every venue in Boston. Adding circles to a Folium map is easy, however there are a couple of drawbacks to going down that path. First, it just looks messy; the cumulative opacity of overlapping circles in areas with high venue density creates areas that are completely dark and blotted out. Second, it results only in viewable decoration on a map, the buffer zone that results isn't a separate geo-object that you can perform operations with (such as programatically checking if coordinates fall within the shadow of the buffer zone, etc). Thus rather than just using Folium to project this buffer zone round venues, we'll create an object representing the buffer zone. Before that, an aside about Coordinate Reference Systems (CRS)...\n",
    "\n",
    "Although Python and Folium do a decent job of not burying you in detail when it's not needed, some geodata transformations do require knowing the EPSG number of the CRS your geodata is projected in. EPSG stands for European Petroleum Survey Group, which no longer exists, but their survey data has evolved into a public repository of spatial reference systems, geodetic datums, etc. EPSG numbers have become the de facto industry standard for labeling coordinate reference systems, used by most GIS systems and libraries, including Folium and GeoPandas.\n",
    "\n",
    "A common standard CRS is EPSG:4326, which uses WGS1984 for both datum and ellipsoid. WGS means it's the World Geodetic System, a coordinate standard used in cartography and navigation which is maintained by the US National Geospatial-Intelligence Agency (NGA). The latest rev is WGS84, first published in 1984 and last revised in 2014. WGS84 is the reference coordinate system used by the Global Positioning System (GPS) and is also the base used by Folium as a default. Unfortunately WGS84 has one drawback when it comes to our buffer zone plans, which I'll get to in a moment.\n",
    "\n",
    "The GeoPandas geometry.buffer method to create buffers around locations uses a single float64 as input, with no means to specify what unit of measure that number refers to. Instead it uses whatever unit of measure is specified by the CRS which the geodata is projected in. Where this becomes problematic is that EPSG:4326 uses degrees as a unit of measure. Degrees of latitude are fairly consistent, 1° equals approximately 69.1 miles. Longitude is another story, 1° of longitude ranges from 69.1 miles at the equator to 0 miles at the poles. At Boston's coordinates, 1° of latitude and longitude is approximately 69 miles wide by 51 miles tall. Given that geometry.buffer only takes one number as the distance parameter, any buffer made in Boston in the WGS84 CRS will be oval shaped. And given the scale of 1°, getting a reasonable buffer of 1000 feet would also be... some really tiny number I'm not going to bother figuring out.\n",
    "\n",
    "GeoPandas supports re-projecting from one CRS to another, so for the purposes of adding the buffer to each point we'll switch to a different CRS. EPSG:2249 uses North American Datum 1983 (NAD83) and ellipsoid GRS 1980. NAD83 is from the US Defense Mapping Agency and uses one survey foot as the standard of measure, which makes getting a 1000 foot buffer from geometry.buffer simply a matter of putting in '1000' as the distance. It isn't necessary to re-project it back from EPSG:2249 to EPSG:4326, as Folium will do that for us when reading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb64ac",
   "metadata": {},
   "source": [
    "The first step is taking a copy of the 'venue_master' dataframe and stripping it to just a few fields, creating a geodataframe from it in CRS EPSG:4326, and converting it to EPSG:2249. This gives us a plot of all the venue locations in a CRS that uses feet as a unit of measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b19ce",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating plot of Venues for CRS transform\n",
    "ven_coords = venue_master[['Venue ID','VenLat','VenLon']]\n",
    "# converts the list of coords in 'ven_coords' into point geometry, object from DataFrame to GeoDataFrame\n",
    "# Note that geopandas does lat/lon backwards from conventional wisdom, as lon(x) and lat(y)\n",
    "ven_coords_gdf = gpd.GeoDataFrame(ven_coords, \n",
    "                                        geometry=gpd.points_from_xy(ven_coords.VenLon, ven_coords.VenLat),\n",
    "                                        crs=4326)\n",
    "# converting the 'gdf' list of point geometry to a CRS that uses feet as UoM\n",
    "ven_coords_gdf_projected2249 = ven_coords_gdf.to_crs(\"epsg:2249\")\n",
    "print('Type:', type(ven_coords_gdf_projected2249), \"/ current CRS is:\",ven_coords_gdf_projected2249.crs)\n",
    "ven_coords_gdf_projected2249.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f78e68",
   "metadata": {},
   "source": [
    "The next step is creating the buffer zone around existing venues using geometry.buffer, then using geometry.unary_union to merge these ~1700 circles into a single multipolygon object. We'll make it projected into a few different CRS in case we need them down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fff30",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating buffer polygons around the items in the list of point geometry, converts from GeoDataFrame to GeoSeries\n",
    "ven_locs_buffer2249 = ven_coords_gdf_projected2249.geometry.buffer(1000) # UoM for 2249 is feet, 820.21ft = 250m\n",
    "ven_locs_buffer32619 = ven_locs_buffer2249.to_crs(epsg=32619)            # UoM for 32619 is meters\n",
    "ven_locs_buffer = ven_locs_buffer2249.to_crs(epsg=4326)                  # UoM for 4326 is degrees\n",
    "print('ven_locs_buffer2249 Interim Type:', type(ven_locs_buffer2249), \"/ current CRS is:\",ven_locs_buffer2249.crs)\n",
    "print('ven_locs_buffer32619 Interim Type:', type(ven_locs_buffer32619), \"/ current CRS is:\",ven_locs_buffer32619.crs)\n",
    "print('ven_locs_buffer (4326) Interim Type:', type(ven_locs_buffer), \"/ current CRS is:\",ven_locs_buffer.crs)\n",
    "# merging the individual circles into a single shapley MultiPolygon\n",
    "ven_locs_buffer_union2249 = ven_locs_buffer2249.geometry.unary_union\n",
    "ven_locs_buffer_union32619 = ven_locs_buffer32619.geometry.unary_union\n",
    "ven_locs_buffer_union = ven_locs_buffer.geometry.unary_union\n",
    "print('ven_locs_buffer_union2249 New Type:', type(ven_locs_buffer_union2249))\n",
    "ven_locs_buffer_union2249"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86aa6d",
   "metadata": {},
   "source": [
    "Then we take the new multipolygon we've created, and convert it back to dataframe then geodataframe, and we're ready to use it in a map layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423bded",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# convert the Shapely MultiPolygon result to DataFrame\n",
    "vlb_union_DF = pd.DataFrame([ven_locs_buffer_union2249]).transpose()\n",
    "vlb_union_DF.rename(columns={0:\"geometry\",}, inplace=True)\n",
    "# make GeoDataFrame from DataFrame, assign CRS\n",
    "vlb_union_GDF = gpd.GeoDataFrame(vlb_union_DF, crs=2249)\n",
    "print('Type:', type(vlb_union_GDF), \"/ current CRS is:\",vlb_union_GDF.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80adc7a",
   "metadata": {},
   "source": [
    "Next we'll take these polygons and all the various bits of venue data, census data, buffer data, etc and create the various map layers and feature groups that will be applied to subsequent maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c870f12",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating map layers\n",
    "# create venue marker map layer\n",
    "fg_VenMarkers = folium.FeatureGroup('Venues',show=False,z_index_offset=1000)\n",
    "for lat, lng, venue, category in zip(venue_master['VenLat'],venue_master['VenLon'],venue_master['Venue'],\n",
    "                                     venue_master['Venue Category']):\n",
    "    label = '{} ({})'.format(venue,category)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=120)\n",
    "    folium.CircleMarker([lat, lng],radius=3,popup=label,color='black',fill=True,fill_color='gray',fill_opacity=0.5,\n",
    "                        parse_html=False,name=\"Venues\").add_to(fg_VenMarkers)\n",
    "# create venue buffer zone map layer\n",
    "VenBuffer = folium.Choropleth(geo_data=vlb_union_GDF,name=\"Buffer Zone\",\n",
    "                              fill_color=\"Gray\",fill_opacity=0.5,line_opacity=0.2,nan_fill_opacity=0,\n",
    "                              control=True,show=False,legend_name=\"Buffer Zone\")\n",
    "# create neighborhood map layer\n",
    "MapHoods = folium.Choropleth(geo_data=hoodgeoJSON, name=\"Neighborhoods\",fill_color=\"lightblue\",fill_opacity=0.5,\n",
    "                             line_color=\"darkblue\",line_opacity=0.9,line_weight=1,nan_fill_opacity=0,\n",
    "                             control=True,show=False,highlight=True,legend_name=\"Neighborhood\")\n",
    "folium.features.GeoJsonPopup(fields=['Neighborhood'],labels=False).add_to(MapHoods.geojson)\n",
    "# create tract map layer\n",
    "MapTract = folium.Choropleth(geo_data=tractgeoJSON, name=\"Census Tracts\",fill_color=\"lightblue\",fill_opacity=0.5,\n",
    "                             line_color=\"blue\",line_opacity=0.9,line_weight=1,nan_fill_opacity=0,\n",
    "                             control=True,show=False,highlight=True,legend_name=\"Census Tracts\")\n",
    "folium.features.GeoJsonPopup(fields=['Neighborhood','Population','Housing Units','NAMELSAD10',\n",
    "                                     'Pop per HU']).add_to(MapTract.geojson)\n",
    "# create population density by tract map layer\n",
    "MapPopDen = folium.Choropleth(geo_data=tractgeoJSON,name=\"Population Density (mi²)\",data=tractgeo,bins=8,\n",
    "                              columns=[\"NAMELSAD10\", \"Population Density\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                              fill_color=\"YlOrRd\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,\n",
    "                              show=False,highlight=True,legend_name=\"Population per Square Mile\")\n",
    "folium.features.GeoJsonPopup(fields=['Population Density','NAMELSAD10']).add_to(MapPopDen.geojson)\n",
    "folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Population Density'],\n",
    "                                                               aliases = ['Population per Square Mile'],labels=True,\n",
    "                                                               sticky=False)).add_to(MapPopDen.geojson)\n",
    "for key in MapPopDen._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopDen._children[key])\n",
    "# create population density by neighborhood map layer\n",
    "MapPopDenH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Population Density (mi²)\",data=hoodgeo,bins=8,\n",
    "                               columns=[\"Neighborhood\", \"Population Density (hood)\"],\n",
    "                               key_on=\"feature.properties.Neighborhood\",fill_color=\"YlOrRd\",\n",
    "                               fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,highlight=True,\n",
    "                               legend_name=\"Population per Square Mile\")\n",
    "folium.features.GeoJsonPopup(fields=['Population Density (hood)','Neighborhood']).add_to(MapPopDenH.geojson)\n",
    "folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Population Density (hood)'],\n",
    "                                                               aliases = ['Population per Square Mile'],labels=True,\n",
    "                                                               sticky=False)).add_to(MapPopDenH.geojson)\n",
    "for key in MapPopDenH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopDenH._children[key])\n",
    "# create population by tract map layer\n",
    "MapPop = folium.Choropleth(geo_data=tractgeoJSON,name=\"Population\",data=tractgeo,bins=8,\n",
    "                           columns=[\"NAMELSAD10\", \"Population\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                           fill_color=\"PuRd\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                           highlight=True,legend_name=\"Population\")\n",
    "folium.features.GeoJsonPopup(fields=['Population','NAMELSAD10']).add_to(MapPop.geojson)\n",
    "folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Population'],aliases = ['Population'],\n",
    "                                                                labels=True,sticky=False)).add_to(MapPop.geojson)\n",
    "for key in MapPop._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPop._children[key])\n",
    "# create population by neighborhood map layer\n",
    "MapPopH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Population\",data=hoodgeo,bins=8,\n",
    "                            columns=[\"Neighborhood\", \"Population (hood)\"],key_on=\"feature.properties.Neighborhood\",\n",
    "                            fill_color=\"PuRd\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                            highlight=True,legend_name=\"Population (hood)\")\n",
    "folium.features.GeoJsonPopup(fields=['Population (hood)','Neighborhood']).add_to(MapPopH.geojson)\n",
    "folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Population (hood)'],\n",
    "                                                               aliases = ['Population'],\n",
    "                                                               labels=True,sticky=False)).add_to(MapPopH.geojson)\n",
    "for key in MapPopH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopH._children[key])\n",
    "# create venues by tract map layer\n",
    "MapVC = folium.Choropleth(geo_data=tractgeoJSON,name=\"Venue Count\",data=tractgeo,bins=8,\n",
    "                          columns=[\"NAMELSAD10\", \"Venue Count\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                          fill_color=\"PuBuGn\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                          highlight=True,legend_name=\"Venue Count\")\n",
    "folium.features.GeoJsonPopup(fields=['Venue Count','NAMELSAD10']).add_to(MapVC.geojson)\n",
    "#folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Count'],aliases = ['Venue Count'],\n",
    "#                                                                labels=True,sticky=False)).add_to(MapVC.geojson)\n",
    "for key in MapVC._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapVC._children[key])\n",
    "# create venues by neighborhood map layer\n",
    "MapVCH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Venue Count\",data=hoodgeo,bins=8,\n",
    "                           columns=[\"Neighborhood\", \"Venue Count (hood)\"],key_on=\"feature.properties.Neighborhood\",\n",
    "                           fill_color=\"PuBuGn\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                           highlight=True,legend_name=\"Venue Count (hood)\")\n",
    "folium.features.GeoJsonPopup(fields=['Venue Count (hood)','Neighborhood']).add_to(MapVCH.geojson)\n",
    "#folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Count (hood)'],\n",
    "#                                                               aliases = ['Venue Count'],\n",
    "#                                                               labels=True,sticky=False)).add_to(MapVCH.geojson)\n",
    "for key in MapVCH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapVCH._children[key])\n",
    "# create venue density by tract map layer\n",
    "MapVD = folium.Choropleth(geo_data=tractgeoJSON,name=\"Venue Density (mi²)\",data=tractgeo,bins=8,\n",
    "                          columns=[\"NAMELSAD10\", \"Venue Density\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                          fill_color=\"RdPu\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                          highlight=True,legend_name=\"Venue Density\")\n",
    "folium.features.GeoJsonPopup(fields=['Venue Density','NAMELSAD10']).add_to(MapVD.geojson)\n",
    "#folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Density'],aliases = ['Venue Density'],\n",
    "#                                                                labels=True,sticky=False)).add_to(MapVD.geojson)\n",
    "for key in MapVD._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapVD._children[key])\n",
    "# create venue density by neighborhood map layer\n",
    "MapVDH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Venue Density (mi²)\",data=hoodgeo,bins=8,\n",
    "                           columns=[\"Neighborhood\", \"Venue Density (hood)\"],key_on=\"feature.properties.Neighborhood\",\n",
    "                           fill_color=\"RdPu\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                           highlight=True,legend_name=\"Venues per Square Mile\")\n",
    "folium.features.GeoJsonPopup(fields=['Venue Density (hood)','Neighborhood']).add_to(MapVDH.geojson)\n",
    "#folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Density (hood)'],\n",
    "#                                                               aliases = ['Venues per Square Mile'],\n",
    "#                                                               labels=True,sticky=False)).add_to(MapVDH.geojson)\n",
    "for key in MapVDH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapVDH._children[key])\n",
    "# create population per venue by tract map layer\n",
    "MapPV = folium.Choropleth(geo_data=tractgeoJSON,name=\"Population per Venue\",data=tractgeo,bins=8,\n",
    "                          columns=[\"NAMELSAD10\", \"Pop per Venue\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                          fill_color=\"OrRd\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                          highlight=True,legend_name=\"Population per Venue\")\n",
    "folium.features.GeoJsonPopup(fields=['Pop per Venue','NAMELSAD10']).add_to(MapPV.geojson)\n",
    "#folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Density'],aliases = ['Venue Density'],\n",
    "#                                                                labels=True,sticky=False)).add_to(MapPV.geojson)\n",
    "for key in MapPV._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPV._children[key])\n",
    "# create population per venue by neighborhood map layer\n",
    "MapPVH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Population per Venue\",data=hoodgeo,bins=8,\n",
    "                          columns=[\"Neighborhood\", \"Pop per Venue (hood)\"],key_on=\"feature.properties.Neighborhood\",\n",
    "                          fill_color=\"OrRd\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                          highlight=True,legend_name=\"Population per Venue\")\n",
    "folium.features.GeoJsonPopup(fields=['Pop per Venue (hood)','Neighborhood']).add_to(MapPVH.geojson)\n",
    "#folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Venue Density (hood)'],\n",
    "#                                                               aliases = ['Venue Density'],\n",
    "#                                                               labels=True,sticky=False)).add_to(MapPVH.geojson)\n",
    "for key in MapPVH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPVH._children[key])\n",
    "# create median income by tract map layer\n",
    "MapPopInc = folium.Choropleth(geo_data=tractgeoJSON,name=\"Median Household Inc\",data=tractgeo,bins=8,\n",
    "                              columns=[\"NAMELSAD10\",\"Median Household Income\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                              fill_color=\"Greens\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                              highlight=True,legend_name=\"Median Household Income\")\n",
    "folium.features.GeoJsonPopup(fields=['Median Household Income','NAMELSAD10']).add_to(MapPopInc.geojson)\n",
    "#folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "#                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "#                        tooltip=folium.features.GeoJsonTooltip(fields=['Median Household Income'],\n",
    "#                                                                aliases = ['Median Household Income'],\n",
    "#                                                                labels=True,sticky=False)).add_to(MapPopInc.geojson)\n",
    "for key in MapPopInc._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopInc._children[key])\n",
    "# create population per housing unit by tract map layer\n",
    "MapPopPerHU = folium.Choropleth(geo_data=tractgeoJSON,name=\"Population per Housing Unit\",data=tractgeo,bins=8,\n",
    "                                columns=[\"NAMELSAD10\",\"Pop per HU\"],key_on=\"feature.properties.NAMELSAD10\",\n",
    "                                fill_color=\"PuBu\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                                highlight=True,legend_name=\"Population per Housing Unit\")\n",
    "folium.features.GeoJsonPopup(fields=['Pop per HU','NAMELSAD10']).add_to(MapPopPerHU.geojson)\n",
    "folium.features.GeoJson(tractgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Pop per HU'],\n",
    "                                                                aliases = ['Population per Housing Unit'],\n",
    "                                                                labels=True,sticky=False)).add_to(MapPopPerHU.geojson)\n",
    "for key in MapPopPerHU._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopPerHU._children[key])\n",
    "# create population per housing unit by neighborhood map layer\n",
    "MapPopPerHUH = folium.Choropleth(geo_data=hoodgeoJSON,name=\"Population per Housing Unit\",data=hoodgeo,bins=8,\n",
    "                                columns=[\"Neighborhood\",\"Pop per HU (hood)\"],key_on=\"feature.properties.Neighborhood\",\n",
    "                                fill_color=\"PuBu\",fill_opacity=0.7,line_opacity=0.2,nan_fill_opacity=0,show=False,\n",
    "                                highlight=True,legend_name=\"Population per Housing Unit (hood)\")\n",
    "folium.features.GeoJsonPopup(fields=['Pop per HU (hood)','Neighborhood']).add_to(MapPopPerHUH.geojson)\n",
    "folium.features.GeoJson(hoodgeoJSON,name='Labels', # adds hover labels\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=['Pop per HU (hood)'],\n",
    "                                                                aliases = ['Population per Housing Unit'],\n",
    "                                                                labels=True,sticky=False)).add_to(MapPopPerHUH.geojson)\n",
    "for key in MapPopPerHUH._children:\n",
    "    if key.startswith('color_map'):\n",
    "        del(MapPopPerHUH._children[key])\n",
    "# create heat map layer\n",
    "VenueArr = venue_master[['VenLatNum', 'VenLonNum']].values\n",
    "fg_heat = plugins.HeatMap(VenueArr, radius=15, name=\"Heat\",overlay=True, control=True, show=True, \n",
    "                          no_wrap=False)\n",
    "# create styles for transit map layers\n",
    "styleT = {'color': 'blue', 'weight': '3', 'opacity': '1'}\n",
    "highlightT = {'color': 'darkblue', 'weight': '2', 'opacity': '1'}\n",
    "styleCR = {'color': 'red', 'weight': '3', 'opacity': '1'}\n",
    "highlightCR = {'color': 'darkred', 'weight': '2', 'opacity': '1'}\n",
    "styleB = {'color': 'purple', 'weight': '3', 'opacity': '1'}\n",
    "highlightB = {'color': 'darkpurple', 'weight': '2', 'opacity': '1'}\n",
    "print(\"...STYLES AND HIGHLIGHTS CREATED\")\n",
    "# create MBTA T map layer\n",
    "fg_MBTA_T = folium.FeatureGroup('MBTA T',show=False,z_index_offset=800)\n",
    "folium.GeoJson(MBTA_RT_Line, name=\"T Lines\", style_function=lambda x: styleT,\n",
    "               highlight_function=lambda x: highlightT,\n",
    "               tooltip=folium.features.GeoJsonTooltip(fields=['Line','Route'])).add_to(fg_MBTA_T)\n",
    "for lat, lng, station, line, route in zip(MBTA_RT_Stop['lat'],MBTA_RT_Stop['lon'],MBTA_RT_Stop['Station'],\n",
    "                                     MBTA_RT_Stop['Line'], MBTA_RT_Stop['Route']):\n",
    "    label = '{} ({} Line, {})'.format(station,line,route)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=150)\n",
    "    folium.CircleMarker([lat, lng],radius=4,popup=label,color='blue',fill=True,fill_color='blue',fill_opacity=0.5,\n",
    "                        parse_html=False,name=\"MBTA T\").add_to(fg_MBTA_T)\n",
    "#create MBTA Commuter Rail map layer\n",
    "fg_MBTA_CR = folium.FeatureGroup('MBTA Commuter Rail',show=False,z_index_offset=700)\n",
    "folium.GeoJson(MBTA_Rail_Line, name=\"CR Lines\", style_function=lambda x: styleCR, \n",
    "               highlight_function=lambda x: highlightCR,\n",
    "               tooltip=folium.features.GeoJsonTooltip(fields=['Line'])).add_to(fg_MBTA_CR)\n",
    "for lat, lng, station, line in zip(MBTA_Rail_Stop['lat'],MBTA_Rail_Stop['lon'],MBTA_Rail_Stop['Station'],\n",
    "                                     MBTA_Rail_Stop['Line']):\n",
    "    label = '{} (Commuter Rail {})'.format(station,line)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=150)\n",
    "    folium.CircleMarker([lat, lng],radius=4,popup=label,color='red',fill=True,fill_color='red',fill_opacity=0.5,\n",
    "                        parse_html=False,name=\"MBTA Commuter Rail\").add_to(fg_MBTA_CR)\n",
    "# create MBTA Bus map layer\n",
    "fg_MBTA_Bus = folium.FeatureGroup('MBTA Bus Routes',show=False,z_index_offset=600)\n",
    "folium.GeoJson(MBTA_Bus_Line, name=\"Bus Lines\", style_function=lambda x: styleB, \n",
    "               highlight_function=lambda x: highlightB,\n",
    "               tooltip=folium.features.GeoJsonTooltip(fields=['Route','Headsign'])).add_to(fg_MBTA_Bus)\n",
    "print(\"...FEATURES CREATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12ce26",
   "metadata": {},
   "source": [
    "First up is the popular \"heat map\", which represents density of venues via hotter colors. Use the layer toggle on the upper left of the map to switch the map tile layer to dark monochrome or standard map if that helps you visualize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaad958",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# heat map\n",
    "heat_map = folium.Map(location=[latitude, longitude], zoom_start=13, control_scale=True, prefer_canvas=True)\n",
    "mono_white = folium.raster_layers.TileLayer(tiles='cartodbpositron', name=\"Light Monochrome\", \n",
    "                                            overlay=False, control=True, show=True, no_wrap=False,\n",
    "                                            opacity=1).add_to(heat_map)             # adding light mono tiles\n",
    "mono_black = folium.raster_layers.TileLayer(tiles='cartodbdark_matter', name=\"Dark Monochrome\", \n",
    "                                            overlay=False, control=True, show=False, no_wrap=False,\n",
    "                                            opacity=1).add_to(heat_map)             # adding dark mono tiles\n",
    "heat_map.add_child(fg_heat).add_child(MapTract).add_child(MapHoods)                 # adding layers\n",
    "heat_map.keep_in_front(fg_heat,MapTract,MapHoods)                                   # toggle to keep heat layer in front\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(heat_map)                    # adding fullscreen toggle\n",
    "folium.LayerControl(position=\"topleft\").add_to(heat_map) \n",
    "heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8ebf2",
   "metadata": {},
   "source": [
    "Below is the \"fat map\", which mushes a whole bunch of layer options into one map. There are layers showing the location of each dining venue in Boston and 1000 foot buffers around each venue, neighborhood and census tract boundaries, T lines and stops, Commuter Rail lines and stops, bus lines, a variety of demographic stats; population, population density, venue count, venue density, population per venue, median household income, and population per housing unit. They get pretty muddled when they're all on, so toggle as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5d9c3",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# map with extra herbs and spices\n",
    "fat_map = folium.Map(location=[latitude, longitude], zoom_start=12, control_scale=True, prefer_canvas=True)\n",
    "fat_map.add_child(MapHoods).add_child(MapTract).add_child(fg_VenMarkers).add_child(VenBuffer).add_child(MapPop)\n",
    "fat_map.add_child(MapPopDen).add_child(MapVC).add_child(MapVD).add_child(MapPV).add_child(MapPopInc)\n",
    "fat_map.add_child(MapPopPerHU).add_child(fg_MBTA_T).add_child(fg_MBTA_CR).add_child(fg_MBTA_Bus)\n",
    "fat_map.keep_in_front(MapHoods,MapTract,fg_MBTA_Bus,fg_MBTA_CR,fg_MBTA_T,MapPopPerHU,MapPopInc,MapPV,MapPopDen,\n",
    "                      MapPop,MapVD,MapVC,VenBuffer,fg_VenMarkers)         # periodically this seems to act in reverse\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(fat_map)           # adding fullscreen toggle\n",
    "folium.LayerControl(position=\"topleft\").add_to(fat_map)                   # adding layer toggle\n",
    "fat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474ebc8",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Analysis</span> <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57363b",
   "metadata": {},
   "source": [
    "Okay, so we've mapped where everything in Boston is, the people, the venues, the transit lines. We've found our starting point, areas which are under-represented in terms of venue per capita. Now it's time to focus in on identifying areas meeting a defined list of criteria. We'll blanket the city with a grid of hexagons surrounding centroids 1000 feet apart. We'll eliminate any centroids that landed within the gray buffer zones we mapped. We'll eliminate centroids that landed in the water (a possibility given that census tracts do contain water areas). We'll eliminate centroids that landed in unlikely areas for placing a restaurant (school grounds, cemeteries, parks, wetlands). Then we'll eliminate any centroids in tracts with population per venue of less than 700. This will give us locations which are:\n",
    "<p>1) not near existing venues\n",
    "<br>2) not in obviously implausible locations\n",
    "<br>3) in parts of the city under-populated with food venues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c8430",
   "metadata": {},
   "source": [
    "### Time For A Hexy Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651f837",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# variables for hex work, X = Lon, Y = Lat\n",
    "maxX = Bos_maxX32619 # Bos_maxX\n",
    "minX = Bos_minX32619 # Bos_minX\n",
    "maxY = Bos_maxY32619 # Bos_maxY\n",
    "minY = Bos_minY32619 # Bos_minY\n",
    "centerX = Bos_centerX32619 # Bos_centerX\n",
    "centerY = Bos_centerY32619 # Bos_centerY\n",
    "BostonPolygonX = BostonPolygon32619 # BostonPolygon\n",
    "x_step = 304.8 # .0025 Distance between adjacent hexagons in the row in UM of CRS (304.8m = 1000ft)\n",
    "print(\"...HEX VARIABLES SET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d6531",
   "metadata": {},
   "source": [
    "The function below will create our evenly spaced centroids, with the requirement that they must be within the overall polygon of the City of Boston we created up near the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04f98d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating a matrix of points within a geography\n",
    "def xy_to_lonlat(x, y):\n",
    "    lonlatTransformer = Transformer.from_crs(32619,4326,always_xy=True) #4326\n",
    "    lonlat = lonlatTransformer.transform(x, y)\n",
    "    return lonlat[0], lonlat[1]\n",
    "k = math.sqrt(3)/2   # Vertical distance coefficient for hexagon grid cells\n",
    "y_step = x_step * k  # Distance between adjacent hexagons in vertical direction\n",
    "lat_result = []      # creating empty list\n",
    "lon_result = []      # creating empty list\n",
    "xs = []              # creating empty list\n",
    "ys = []              # creating empty list\n",
    "num_columns = int((maxX - minX)/x_step) + 1\n",
    "num_rows = int((maxY - minY)/y_step) + 1\n",
    "for i in range(0, num_rows):\n",
    "    y = minY + i * y_step\n",
    "    x_offset = x_step/2 if i%2==0 else 0\n",
    "    for j in range(0, num_columns):\n",
    "        x = minX + j * x_step + x_offset\n",
    "        lon = x\n",
    "        lat = y\n",
    "#        lon, lat = xy_to_lonlat(x, y)\n",
    "        if boolean_point_in_polygon(Feature(geometry=Point([lon,lat])),Feature(geometry=BostonPolygonX)):\n",
    "            lat_result.append(lat)\n",
    "            lon_result.append(lon)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "print(\"...\",len(lat_result),'POINTS CREATED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0197e",
   "metadata": {},
   "source": [
    "Next we'll create a function to build the hexagonal polygons around a given set of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa349cc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating hexagon function\n",
    "def lonlat_to_xy(lon, lat):\n",
    "    XYTransformer = Transformer.from_crs(4326,32619,always_xy=True) #4326\n",
    "    xy = XYTransformer.transform(lon, lat)\n",
    "    return xy[0], xy[1]\n",
    "def get_hexagon_coordinates(centroid_latlon, side_length_in_meters):\n",
    "    unit_hexagon_vertices_rotate = np.array([[0, 1], [-math.sqrt(3)/2, 1/2], [-math.sqrt(3)/2, -1/2],\n",
    "                                       [0, -1], [math.sqrt(3)/2, -1/2], [math.sqrt(3)/2, 1/2]])\n",
    "#    x, y = lonlat_to_xy(centroid_latlon[1], centroid_latlon[0])        # takes lon/lat and changes to XY\n",
    "    x, y = (centroid_latlon[1], centroid_latlon[0])                     # skips conversion    \n",
    "    vertices = unit_hexagon_vertices_rotate\n",
    "    vertices *= side_length_in_meters\n",
    "    vertices = [xy_to_lonlat(v[0]+x, v[1]+y) for v in vertices]\n",
    "    return vertices\n",
    "print(\"...FUNCTIONS CREATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84016332",
   "metadata": {},
   "source": [
    "And next we use the function to create the polygons using the list of centroids we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4e2ff",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating hexagons\n",
    "a = x_step/math.sqrt(3)\n",
    "hexagons = []\n",
    "for latlon in zip(lat_result, lon_result):\n",
    "    vertices = get_hexagon_coordinates(latlon, a) # had been 'a'\n",
    "    hexagons.append(Polygon(vertices))\n",
    "print(\"...\",len(hexagons),'HEXAGONS CREATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528309a7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with hex results\n",
    "# turning lat, lon, x, y results into DF as floats\n",
    "lat_resultDF = pd.DataFrame([lat_result]).transpose().astype(float)\n",
    "lat_resultDF.rename(columns={0:\"lat (y)\",}, inplace=True)\n",
    "lon_resultDF = pd.DataFrame([lon_result]).transpose().astype(float)\n",
    "lon_resultDF.rename(columns={0:\"lon (x)\",}, inplace=True)\n",
    "xsDF = pd.DataFrame([xs]).transpose().astype(float)\n",
    "xsDF.rename(columns={0:\"x\",}, inplace=True)\n",
    "ysDF = pd.DataFrame([ys]).transpose().astype(float)\n",
    "ysDF.rename(columns={0:\"y\",}, inplace=True)\n",
    "# merging DFs by index\n",
    "intDF1=lon_resultDF.merge(lat_resultDF, how='outer', left_index=True, right_index=True)\n",
    "intDF2=intDF1.merge(xsDF, how='outer', left_index=True, right_index=True)\n",
    "MergePointsDF=intDF2.merge(ysDF, how='outer', left_index=True, right_index=True)\n",
    "# converting DF to GDF\n",
    "MergePointsGDF=gpd.GeoDataFrame(MergePointsDF, geometry=gpd.points_from_xy(MergePointsDF.x, MergePointsDF.y),crs=32619)\n",
    "MergePointsGDF.drop(columns=['x', 'y'], inplace=True)\n",
    "HexStep1 = MergePointsGDF.shape\n",
    "MergePointsGDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6576d4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# making GeoJSON from hexes\n",
    "HexOne_DF = pd.DataFrame([hexagons]).transpose()\n",
    "HexOne_DF.rename(columns={0:\"geometry\",}, inplace=True)\n",
    "# make GeoDataFrame from DataFrame, assign CRS\n",
    "HexOne_GDF = gpd.GeoDataFrame(HexOne_DF, crs=2249)\n",
    "# make JSON from GDF\n",
    "HexOne_GDFJSON = HexOne_GDF.to_json()\n",
    "HexOne_GDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043ebc1",
   "metadata": {},
   "source": [
    "Below is our baseline hexmap of Boston, without any of the aforementioned exclusions applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b18154",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# preliminary hex map\n",
    "hexone_map = folium.Map(location=[42.31953, -71.08097], zoom_start=12, control_scale=True, prefer_canvas=True)\n",
    "fg_testhex = folium.GeoJson(HexOne_GDFJSON,show=True,name=\"Hexagons\").add_to(hexone_map)\n",
    "hexone_map.add_child(MapHoods).add_child(MapTract).add_child(VenBuffer).add_child(fg_VenMarkers)\n",
    "hexone_map.keep_in_front(MapHoods,MapTract,VenBuffer,fg_testhex,fg_VenMarkers)\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(hexone_map)\n",
    "folium.LayerControl(position=\"topleft\").add_to(hexone_map) \n",
    "hexone_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99a2b7",
   "metadata": {},
   "source": [
    "### Removing Points in Buffer from Hex List\n",
    "Now it's time to start carving chunks out of the master hexmap we just made, based on the rules we've laid out. The first step is to take each centroid in the grid and check if it is within the buffer zone multipolygon we made earlier. A new column will be added to the point list to tell us if a given row is in the buffer or not, then we'll remove the offending rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf8118",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checking for centroids in the venue buffer zone\n",
    "InBuffer = []        # creating empty list\n",
    "for lon, lat in zip(MergePointsGDF['lon (x)'], MergePointsGDF['lat (y)']):\n",
    "    x = lon #= x\n",
    "    y = lat #= y\n",
    "#    lon, lat = xy_to_lonlat(x, y)\n",
    "    if boolean_point_in_polygon(Feature(geometry=Point([lon,lat])),Feature(geometry=ven_locs_buffer_union32619)):\n",
    "        InBuffer.append([lat,lon,x,y])\n",
    "print(\"...POINTS IN BUFFER IDENTIFIED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665f781",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# taking buffer check results and putting into GDF\n",
    "InBufferDF = pd.DataFrame(InBuffer)\n",
    "InBufferDF.rename(columns={0:\"lat (y)\",1:\"lon (x)\",2:\"x\",3:\"y\"}, inplace=True)\n",
    "InBufferGDF32619=gpd.GeoDataFrame(InBufferDF, geometry=gpd.points_from_xy(InBufferDF.x, InBufferDF.y),crs=32619)\n",
    "InBufferGDF32619.drop(columns=['x', 'y'], inplace=True)\n",
    "InBufferGDF32619['in buffer'] = 'yes'\n",
    "InBufferGDF32619.reset_index(inplace=True, drop=True)\n",
    "print(\"MergePointsGDF:\",MergePointsGDF.shape,\" / InBufferGDF32619:\",InBufferGDF32619.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad809482",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# working with \"in buffer\" results\n",
    "# merging original point list with \"in buffer\" point list\n",
    "MergePointsF1GDF32619=MergePointsGDF.merge(InBufferGDF32619, how='outer', on='geometry')\n",
    "MergePointsF1GDF32619.drop(columns=['lat (y)_y', 'lon (x)_y'], inplace=True)\n",
    "MergePointsF1GDF32619.rename(columns={\"lat (y)_x\":\"lat (y)\",\"lon (x)_x\":\"lon (x)\"}, inplace=True)\n",
    "MergePointsF1GDF32619.reset_index(inplace=True, drop=True)\n",
    "# dropping \"in buffer = yes\" rows, then dropping \"in buffer\" column\n",
    "MergePointsF1GDF32619.drop(MergePointsF1GDF32619.loc[MergePointsF1GDF32619['in buffer']=='yes'].index,inplace=True)\n",
    "MergePointsF1GDF32619.drop(columns=['in buffer'], inplace=True)\n",
    "MergePointsF1GDF32619.reset_index(inplace=True, drop=True)\n",
    "# converting to CRS 4326 for census tract check\n",
    "MergePointsF1GDF = MergePointsF1GDF32619.to_crs(epsg=4326)\n",
    "# copying geometry coords to their own columns\n",
    "MergePointsF1GDF['lon'] = MergePointsF1GDF['geometry'].x\n",
    "MergePointsF1GDF['lat'] = MergePointsF1GDF['geometry'].y\n",
    "MergePointsF1GDF=MergePointsF1GDF[['lon','lon (x)','lat','lat (y)','geometry']]\n",
    "HexStep2 = MergePointsF1GDF.shape\n",
    "print('MergePointsF1GDF (4326) Type:', type(MergePointsF1GDF), \"/ current CRS is:\",MergePointsF1GDF.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed12f4",
   "metadata": {},
   "source": [
    "### Checking Census Tract of Hex List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a1bcf",
   "metadata": {},
   "source": [
    "The Census Bureau uses tract codes in the 9800 range to identify special land-use areas with minimal population, such as parks, wetlands, nature preserves, cemeteries, airports, industrial land, etc. The Boston data bears this out:\n",
    "<p>9801.01 Harbor Islands\n",
    "<br>9803 Franklin Park\n",
    "<br>9807 Stony Brook Reservation\n",
    "<br>9810 Arnold Arboretum\n",
    "<br>9811 Forest Hills Cemetery\n",
    "<br>9812.01 Marine Park and Castle Island\n",
    "<br>9812.02 Massport Terminal\n",
    "<br>9813 Logan Airport\n",
    "<br>9815.01 Charles River Reservation\n",
    "<br>9815.02 Irving Oil Terminal\n",
    "<br>9816 Belle Isle Marsh Reservation\n",
    "<br>9817 Boston Commons\n",
    "<br>9818 Muddy River and Jamaica Pond\n",
    "\n",
    "Of these only the Logan Airport tract has a significant restaurant density, however the entire tract is controlled by Massport, and the 80+ venues (including nine Dunkin' shops) are all in the various terminals of the airport. Definitely doesn't meet our \"not near existing venues\" rule. So we'll remove the hexagon centroids that are in the 98XX tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc85e5d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create \"getCensusHexInfo\" function with Lat+Lng from input \"MergePointsF1GDF\"\n",
    "# determines what Census Tract each hex point is located in\n",
    "# benchmark (4) = Public_AR_Current, vintage (410) = Census2010_Current\n",
    "def getCensusHexInfo(lons,lonx,lats,laty,geo):\n",
    "    census_list=[]\n",
    "    for lng,lnx,lat,lay,geom in zip(lons,lonx,lats,laty,geo):\n",
    "        # print(ven_id)\n",
    "        url_batch_geocode = 'https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={}&y={}&benchmark=4&vintage=410&format=json'.format(\n",
    "            lng,lat)\n",
    "        census_results = requests.get(url_batch_geocode).json()['result']['geographies']['Census Tracts']\n",
    "        census_list.append([(lng,lnx,lat,lay,geom,v['POP100'],v['GEOID'],v['INTPTLAT'],v['NAME'],v['INTPTLON'],\n",
    "                             v['HU100']) for v in census_results])\n",
    "    census_info = pd.DataFrame([item for loc_list in census_list for item in loc_list])\n",
    "    census_info.columns = ['lon','lon (x)','lat','lat (y)','geometry','Population','GEOID','INTPTLAT',\n",
    "                           'NAME','INTPTLON','Housing_Units']\n",
    "    return(census_info)\n",
    "print(\"...'getCensusInfo' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e182a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# running getCensusHexInfo\n",
    "hex_census_raw=getCensusHexInfo(lons=MergePointsF1GDF['lon'],lonx=MergePointsF1GDF['lon (x)'],\n",
    "                                lats=MergePointsF1GDF['lat'],laty=MergePointsF1GDF['lat (y)'],\n",
    "                                geo=MergePointsF1GDF['geometry'])\n",
    "print('...CENSUS TRACT RETRIEVAL COMPLETE')\n",
    "HexStep3 = hex_census_raw.shape\n",
    "hex_census_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b71fa8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hex census cleanup\n",
    "hex_census=hex_census_raw[['GEOID','NAME','Population','Housing_Units','INTPTLAT','INTPTLON','lon','lon (x)',\n",
    "                           'lat','lat (y)','geometry']]\n",
    "hex_census.rename(columns={\"GEOID\":\"GEOID10\",\"NAME\":\"NAMELSAD10\"}, inplace=True) # renaming cols\n",
    "hex_census.drop(columns=['INTPTLAT','INTPTLON'], inplace=True)                   # dropping unneeded cols\n",
    "hex_census.drop(hex_census.loc[hex_census['Population']==0].index,inplace=True)  # dropping 0 pop rows\n",
    "hex_census = hex_census[~hex_census.GEOID10.str.startswith(('2502598'))]         # dropping rows in 98XX tracts\n",
    "hex_census = hex_census[~hex_census.GEOID10.str.startswith(('250214'))]          # dropping a couple on the border\n",
    "hex_census.reset_index(inplace=True, drop=True)\n",
    "HexStep4 = hex_census.shape\n",
    "hex_census.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedc0e0",
   "metadata": {},
   "source": [
    "Using the census tract ID we've pulled, we can tie back to the various bits of demographic etc data we previously assembled, and based on that we'll remove any centroids that are in census tracts below 700 people per venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022b377",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# merging hex data with tract info via identified census tract\n",
    "hex_tract_info = pd.merge(hex_census, tractgeo, how='left', on='NAMELSAD10')\n",
    "# dropping cols\n",
    "hex_tract_info.drop(columns=['Population_y','Housing_Units','FID','OBJECTID','STATEFP10','State Name','COUNTYFP10',\n",
    "                             'County Name','TRACTCE10','GEOID10_y','MTFCC10','FUNCSTAT10','ALAND10','AWATER10',\n",
    "                             'INTPTLAT10','INTPTLON10','Shape_STAr','Shape_STLe','Shape__Area','Shape__Length',\n",
    "                             'geometry_y','NAME10'], inplace=True)\n",
    "# renaming cols\n",
    "hex_tract_info.rename(columns={\"GEOID10_x\":\"GEOID10\",\"Population_x\":\"Population\",\"geometry_x\":\"geometry (4326)\",\n",
    "                               \"lat\":\"lat (4326)\",\"lon\":\"lon (4326)\",\"lat (y)\":\"y\",\"lon (x)\":\"x\"}, inplace=True)\n",
    "HexStep5 = hex_tract_info.shape\n",
    "hex_info=hex_tract_info[['GEOID10','NAMELSAD10','Neighborhood','Population','Population Density','Housing Units',\n",
    "                         'Pop per HU','Venue Count','Venue Density','Pop per Venue','Median Household Income',\n",
    "                         'lon (4326)','x','lat (4326)','y','geometry (4326)']]\n",
    "hex_info.reset_index(inplace=True, drop=True)\n",
    "HexStep6 = hex_info.shape\n",
    "hex_info.drop(hex_info.loc[hex_info['Pop per Venue']<700].index,inplace=True)  # dropping low ppv rows\n",
    "hex_info.reset_index(inplace=True, drop=True)\n",
    "HexStep7 = hex_info.shape\n",
    "print(\"shape changes:\",HexStep1,\"/\",HexStep2,\"/\",HexStep3,\"/\",HexStep4,\"/\",HexStep5,\"/\",HexStep6,\"/\",HexStep7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a46878",
   "metadata": {},
   "source": [
    "We started with 1816 centroids, removing those in the buffer zone dropped it to 819, removing those in non-viable tracts dropped it to 460, and limiting to those with Pop per Ven of 700+ dropped it to 253. Let's take a quick look to check if any of the remaining centroids are sketchy, eg. just off shore, in a park or schoolyard, etc. We'll map our remaining centroids and take a quick look at where the centroids landed. For example, that zig-zag of four points in the river just south of the airport... those should probably go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952c109",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hex map with buffer violations removed\n",
    "hextemp_map=folium.Map(location=[42.31953, -71.08097],zoom_start=12,control_scale=True,prefer_canvas=True)\n",
    "fg_hexmarkers = folium.FeatureGroup('Markers',show=True,z_index_offset=500)\n",
    "for lat, lng, Col1, Col2 in zip(hex_info['lat (4326)'], hex_info['lon (4326)'],hex_info['x'], hex_info['y']):\n",
    "    label = '{} ({})'.format(Col1,Col2)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=120)\n",
    "    folium.CircleMarker([lat, lng],radius=4,popup=label,color='black',fill=True,fill_color='gray',fill_opacity=0.5,\n",
    "                        parse_html=False,name=\"Markers\").add_to(fg_hexmarkers)\n",
    "    fg_hexmarkers.add_to(hextemp_map)\n",
    "hextemp_map.add_child(MapHoods).add_child(MapTract)\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(hextemp_map)\n",
    "folium.LayerControl(position=\"topleft\").add_to(hextemp_map) \n",
    "hextemp_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb4f35",
   "metadata": {},
   "source": [
    "As one would expect, the matrix of centroids we generated has some \"problem children\". The coordinates for those locs have been passed to a separate list and next we'll split the centroids into \"ok\" and \"problem\" for one final inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d885f76",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \"problem hex\" work\n",
    "problem_hex[['b1', 'b2']] = problem_hex['geometry (4326)'].str.strip('POINT ') \\\n",
    "                              .str.strip('()')                                 \\\n",
    "                              .str.split(' ', expand=True)\n",
    "hex_info[['geotext']]=hex_info['geometry (4326)'].astype(str)\n",
    "hex_info[['b1', 'b2']] = hex_info['geotext'].str.strip('POINT ') \\\n",
    "                              .str.strip('()')                   \\\n",
    "                              .str.split(' ', expand=True)\n",
    "hex_info_fix = pd.merge(hex_info, problem_hex, how='left', left_on=['b1','b2'], right_on = ['b1','b2'])\n",
    "hex_info_fix.drop(columns=['geotext','b1','b2','GEOID10_y','NAMELSAD10_y','Neighborhood_y','Population_y',\n",
    "                           'Population Density_y','Housing Units_y','Pop per HU_y','Venue Count_y','Venue Density_y',\n",
    "                           'Pop per Venue_y','Median Household Income_y','lon (4326)_y','x_y','lat (4326)_y','y_y',\n",
    "                           'geometry (4326)_y'], inplace=True)\n",
    "hex_info_fix.rename(columns={\"GEOID10_x\":\"GEOID10\",\"NAMELSAD10_x\":\"NAMELSAD10\",\"Neighborhood_x\":\"Neighborhood\",\n",
    "                             \"Population_x\":\"Population\",\"Population Density_x\":\"Population Density\",\n",
    "                             \"Housing Units_x\":\"Housing Units\",\"Pop per HU_x\":\"Pop per HU\",\n",
    "                             \"Venue Count_x\":\"Venue Count\",\"Venue Density_x\":\"Venue Density\",\n",
    "                             \"Pop per Venue_x\":\"Pop per Venue\",\"Median Household Income_x\":\"Median Household Income\",\n",
    "                             \"lon (4326)_x\":\"lon (4326)\",\"x_x\":\"x\",\"lat (4326)_x\":\"lat (4326)\",\"y_x\":\"y\",\n",
    "                             \"geometry (4326)_x\":\"geometry (4326)\",\"geometry_x\":\"geometry\"}, inplace=True)\n",
    "hex_info_fix.reset_index(inplace=True, drop=True)\n",
    "problemdots = hex_info_fix[(hex_info_fix.Status == 'problem')]\n",
    "problemdots.reset_index(inplace=True, drop=True)\n",
    "okdots = hex_info_fix[(hex_info_fix.Status == 'ok')]\n",
    "okdots.reset_index(inplace=True, drop=True)\n",
    "print(\"problem_hex\",problem_hex.shape,\"/ hex_info\",hex_info.shape,\"/ hex_info_fix\",hex_info_fix.shape,\"/ okdots\",\n",
    "      okdots.shape,\"/ problemdots\",problemdots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e989ad",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checking for remaining issues, should be no NaN status entries\n",
    "hex_info_fix.groupby('Status', dropna=False)[[\"GEOID10\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5742df8",
   "metadata": {},
   "source": [
    "Of our 253 remaining centroids, 72 are in places like schoolyards, marshland, etc. Dropping those leaves 181 viable centroids remaining. In the map below, the bad ones are in red, the survivors are in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a22a28",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating map of problem hex work\n",
    "hexfix_map = folium.Map(location=[42.31953, -71.08097], zoom_start=12, control_scale=True, prefer_canvas=True)\n",
    "fg_prob = folium.FeatureGroup('Problem Dots',show=True,z_index_offset=1000)\n",
    "for lat, lng, Col1, Col2 in zip(problemdots['lat (4326)'], problemdots['lon (4326)'],problemdots['x'],problemdots['y']):\n",
    "    label = '{} ({})'.format(Col1,Col2)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=120)\n",
    "    folium.CircleMarker([lat, lng],radius=4,popup=label,color='darkred',fill=True,fill_color='red',fill_opacity=0.5,\n",
    "                        parse_html=False,name=\"Markers\").add_to(fg_prob)\n",
    "    fg_prob.add_to(hexfix_map)\n",
    "fg_ok = folium.FeatureGroup('OK Dots',show=True,z_index_offset=1000)\n",
    "for lat, lng, Col1, Col2 in zip(okdots['lat (4326)'], okdots['lon (4326)'],okdots['x'], okdots['y']):\n",
    "    label = '{} ({})'.format(Col1,Col2)\n",
    "    label = folium.Popup(label, parse_html=True, max_width=120)\n",
    "    folium.CircleMarker([lat, lng],radius=4,popup=label,color='darkgreen',fill=True,fill_color='green',\n",
    "                        fill_opacity=0.5,parse_html=False,name=\"Markers\").add_to(fg_ok)\n",
    "    fg_ok.add_to(hexfix_map)\n",
    "hexfix_map.add_child(MapHoods).add_child(MapTract).add_child(fg_hexmarkers)\n",
    "hexfix_map.keep_in_front(MapHoods,MapTract,fg_hexmarkers,fg_prob,fg_ok)\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(hexfix_map)\n",
    "folium.LayerControl(position=\"topleft\").add_to(hexfix_map) \n",
    "hexfix_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabe5b2",
   "metadata": {},
   "source": [
    "Next we'll take our 181 remaining centroids and use K-Means clustering to group them. First we'll chart a series of runs using K=1 thru K=20 to determine how many clusters to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0015dbc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# testing K values for KMeans clustering\n",
    "locs_for_km = okdots[['lon (4326)','lat (4326)']]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # code to suppress unnecessary memory leaks warning\n",
    "\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 20)\n",
    "for k in K: # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(locs_for_km)\n",
    "    kmeanModel.fit(locs_for_km)\n",
    "    distortions.append(sum(np.min(cdist(locs_for_km, kmeanModel.cluster_centers_,'euclidean'), \n",
    "                                  axis=1)) / locs_for_km.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "    mapping1[k] = sum(np.min(cdist(locs_for_km, kmeanModel.cluster_centers_,'euclidean'), \n",
    "                             axis=1)) / locs_for_km.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method for K Determination')\n",
    "plt.locator_params(axis=\"x\", nbins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4c44c",
   "metadata": {},
   "source": [
    "It's not the most clear-cut elbow in the world, but at 7 there's a decent flattening, so we'll set our K at 7 and make our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e117612",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# running KMeans based on K=7\n",
    "kclusters = 7\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(locs_for_km)\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd897bc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# identifying the centroid of the resulting clusters\n",
    "cluster_centroids = [(cc[0], cc[1]) for cc in kmeans.cluster_centers_]\n",
    "cluster_centers = pd.DataFrame(cluster_centroids)\n",
    "cluster_centers.rename(columns={0:\"lon\",1:\"lat\"}, inplace=True)\n",
    "cluster_centers.reset_index(inplace=True, drop=False)\n",
    "cluster_centers.rename(columns={\"index\":\"ClusterN\"}, inplace=True)\n",
    "cluster_centers['Cluster']=cluster_centers['ClusterN']+1\n",
    "cluster_centers.drop(columns=['ClusterN'], inplace=True)\n",
    "cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c499f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# adding cluster numbers to hex centroid list\n",
    "locs_for_km.insert(0, \"Cluster_Labels\", kmeans.labels_)\n",
    "ok_clustered = pd.merge(okdots, locs_for_km, how='outer', left_on=['lon (4326)','lat (4326)'], \n",
    "                        right_on = ['lon (4326)','lat (4326)'])\n",
    "ok_clustered['Cluster']=ok_clustered['Cluster_Labels']+1\n",
    "cols = ok_clustered.columns.tolist()                    # define a list of column names\n",
    "cols.insert(0, cols.pop(cols.index('Cluster')))         # move the column name to the beggining\n",
    "ok_clustered = ok_clustered.reindex(columns= cols)      # then use .reindex() function to reorder\n",
    "ok_clustered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45059afc",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating final hex centroid geodataframe\n",
    "HexDotsFinalGDF=gpd.GeoDataFrame(ok_clustered, geometry=gpd.points_from_xy(ok_clustered.x, ok_clustered.y),crs=32619)\n",
    "print(\"HexDotsFinalGDF\",'Type:',type(HexDotsFinalGDF),\"/ current CRS is:\",HexDotsFinalGDF.crs,\"/ shape:\",\n",
    "      HexDotsFinalGDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d162f8",
   "metadata": {},
   "source": [
    "### Making The Final Hexagons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4507e",
   "metadata": {},
   "source": [
    "Okay, we've got our list of centroids filtered down to 181 and assigned to one of seven clusters, now it's time to revist our hexagon function and make our final hexagons..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e00bb4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating final hexagons\n",
    "a = x_step/math.sqrt(3)\n",
    "final_hexagons = []\n",
    "for latlon in zip(HexDotsFinalGDF.y,HexDotsFinalGDF.x):\n",
    "    vertices = get_hexagon_coordinates(latlon, a)\n",
    "    final_hexagons.append(Polygon(vertices))\n",
    "print(\"...\",len(final_hexagons),'FINAL HEXAGONS CREATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde75a78",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# transpose hexes into DataFrame, then GeoDataFrame, then GeoJSON\n",
    "HexTwo_DF = pd.DataFrame([final_hexagons]).transpose()\n",
    "HexTwo_DF.rename(columns={0:\"geometry\",}, inplace=True)\n",
    "# make GeoDataFrame from DataFrame, assign CRS\n",
    "HexTwo_GDF = gpd.GeoDataFrame(HexTwo_DF, crs=2249)\n",
    "# make JSON from GDF\n",
    "HexTwo_GDFJSON = HexTwo_GDF.to_json()\n",
    "HexTwo_GDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612c59d",
   "metadata": {},
   "source": [
    "Below are our final clusters and hexes. There are a variety of other layers you can toggle, neighborhood and tract boundaries, transit lines, and all the demographic layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b7d54",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# final hex map\n",
    "hextwo_map = folium.Map(location=[42.31953, -71.08097], zoom_start=12, control_scale=True, prefer_canvas=True)\n",
    "x = np.arange(kclusters) \n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "markers_colors = []\n",
    "fg_centmark = folium.FeatureGroup('Centroids',show=True,z_index_offset=1000)\n",
    "for lat, lon, cluster in zip(HexDotsFinalGDF['lat (4326)'], HexDotsFinalGDF['lon (4326)'], HexDotsFinalGDF['Cluster']):\n",
    "    label = folium.Popup('Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker([lat, lon],radius=5,popup=label,color=rainbow[int(cluster)-1],fill=True,\n",
    "                        fill_color=rainbow[int(cluster)-1],fill_opacity=0.7,control=True,show=True,\n",
    "                       name=\"Centroids\").add_to(fg_centmark)   \n",
    "fg_hex = folium.GeoJson(HexTwo_GDFJSON,name=\"Hexagons\",control=True,show=True)\n",
    "fg_clusters = folium.FeatureGroup('Cluster Centroids',show=True)\n",
    "for lat, lon, cluster in zip(cluster_centers['lat'], cluster_centers['lon'], cluster_centers['Cluster']):\n",
    "    label = folium.Popup('Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.Circle([lat, lon],radius=1600,popup=label,color='gray',fill=True,\n",
    "                        fill_color='gray',fill_opacity=0.3,control=True,show=True,\n",
    "                       name=\"Cluster Centroids\").add_to(fg_clusters)\n",
    "hextwo_map.add_child(fg_hex).add_child(fg_centmark).add_child(fg_clusters).add_child(fg_VenMarkers)\n",
    "hextwo_map.add_child(MapHoods).add_child(MapTract).add_child(VenBuffer).add_child(MapPop).add_child(MapPopDen)\n",
    "hextwo_map.add_child(MapVC).add_child(MapVD).add_child(MapPV).add_child(MapPopInc).add_child(MapPopPerHU)\n",
    "hextwo_map.add_child(fg_MBTA_T).add_child(fg_MBTA_CR).add_child(fg_MBTA_Bus)\n",
    "hextwo_map.keep_in_front(MapHoods,MapTract,VenBuffer,MapPopPerHU,MapPopInc,MapPV,MapPopDen,MapPop,MapVD,MapVC,\n",
    "                         fg_MBTA_Bus,fg_MBTA_CR,fg_MBTA_T,fg_clusters,fg_centmark,fg_hex,fg_VenMarkers)\n",
    "plugins.Fullscreen(position=\"topleft\",title=\"Fullscreen\",title_cancel=\"Exit Fullscreen\",\n",
    "                   force_separate_button=True,).add_to(hextwo_map)\n",
    "folium.LayerControl(position=\"topleft\").add_to(hextwo_map) \n",
    "hextwo_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3802ed0",
   "metadata": {},
   "source": [
    "As expected when we first made the \"Population per Venue\" bar chart, the neighborhoods in the southwest of the city dominate. There are several areas in the these clusters which are adjacent to T/Commuter Rail stations, which is another plus. Before moving on to results and conclusion, we'll take a quick look at what sort of venues are currently present in our target areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bd9b3",
   "metadata": {},
   "source": [
    "### One-Hot Encoding on Categories by Census Tract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d9fa9",
   "metadata": {},
   "source": [
    "For purposes of ranking category types within tracts, I decided to use top three as the number of venue categories to use. In previous one-hot encoding exercises we'd gone as far as using the top 10, however looking at the number of different categories in each tract, I found that going that deep introduced more bad data than it was worth.\n",
    "\n",
    "The way the standard \"most common category\" function works, once it runs out of venues and/or distinct categories, it starts filling in empty categories alphabetically. This means that if a tract only has three different categories and you ask for the five most common, number 4 and number 5 aren't real. This became apparent when running the function with \"num_top_venues\" at 10 and finding that \"African\" made the top ten for quite a few census tracts even though there are only seven African restaurants in the entire city. Looking at the tracts, 87% of tracts have less than 10 distinct categories, 44% of tracts have less than 5 distinct categories, and 23% of tracts have less than 3 distinct categories. So \"top ten\" is definitely out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a1990",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# one hot encoding, \"freq\" is percent of venues in the neighborhood\n",
    "# 'get_dummies' converts categorical variable into dummy/indicator variables.\n",
    "vencat_onehot = pd.get_dummies(venue_master[['Broad Category']], prefix=\"\", prefix_sep=\"\")\n",
    "#vencat_onehot = pd.get_dummies(venue_master[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "vencat_onehot.head()\n",
    "# add neighborhood data back to onehot dataframe\n",
    "vencat_onehot['NAMELSAD10'] = venue_master['NAMELSAD10']\n",
    "# move re-inserted column back to first position\n",
    "cols = vencat_onehot.columns.tolist()                    # define a list of column names\n",
    "cols.insert(0, cols.pop(cols.index('NAMELSAD10')))       # move the column name to the beggining\n",
    "vencat_onehot = vencat_onehot.reindex(columns= cols)     # then use .reindex() function to reorder\n",
    "# grouping\n",
    "vencat_grouped = vencat_onehot.groupby('NAMELSAD10').mean().reset_index()\n",
    "vencat_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fd8f6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# create most common ven function\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    return row_categories_sorted.index.values[0:num_top_venues]\n",
    "print(\"...'return_most_common_venues' FUNCTION CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe62b0c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# find most common venues for each tract\n",
    "num_top_venues = 3\n",
    "# adds 'st' to '1st' etc\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "# create columns according to number of top venues\n",
    "columns = ['NAMELSAD10']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try: columns.append('{}{} Most Common Cat'.format(ind+1, indicators[ind]))\n",
    "    except: columns.append('{}th Most Common Cat'.format(ind+1))\n",
    "# create a new dataframe\n",
    "NAMELSAD10_venues_sorted = pd.DataFrame(columns=columns)\n",
    "NAMELSAD10_venues_sorted['NAMELSAD10'] = vencat_grouped['NAMELSAD10']\n",
    "for ind in np.arange(vencat_grouped.shape[0]):\n",
    "    NAMELSAD10_venues_sorted.iloc[ind, 1:] = return_most_common_venues(vencat_grouped.iloc[ind, :], num_top_venues)\n",
    "NAMELSAD10_venues_sorted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c054d2",
   "metadata": {},
   "source": [
    "The rows below list the three most common venue types, along with some demographic basics, for any census tract which hosts one of our 181 final points. You'll notice a handful of tracts that have no venues at all, which seems like an opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5679333",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# find most common venues for census tracts in the hex zones\n",
    "OK1H = pd.DataFrame(ok_clustered, copy=True)\n",
    "OK1H.drop(columns=['lon (4326)','x','lat (4326)','y','geometry (4326)','geometry','Cluster_Labels'], inplace=True)\n",
    "# the \"subset\" parameter specifies which columns to match on, otherwise it matches on all fields\n",
    "OK1H.drop_duplicates(subset=['Cluster','GEOID10','Status'], keep='first', ignore_index=True,inplace=True)\n",
    "OK1H.sort_values(by=['Cluster','Neighborhood','GEOID10'],ignore_index=True, inplace=True)\n",
    "OK1H_Tops = pd.merge(OK1H, NAMELSAD10_venues_sorted, how='left', on='NAMELSAD10')\n",
    "OK1H.drop(columns=['Pop per HU','Status'], inplace=True)\n",
    "ExistingTypes = OK1H_Tops[['Cluster','Neighborhood','NAMELSAD10','1st Most Common Cat','2nd Most Common Cat',\n",
    "                           '3rd Most Common Cat','Population','Venue Count','Pop per Venue','Population Density',\n",
    "                           'Venue Density','Housing Units','Median Household Income']]\n",
    "ExistingTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc54306",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Results and Discussion</span> <a name=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3862a49",
   "metadata": {},
   "source": [
    "The analysis shows that the tourist-heavy northeast of the city around the North End and Downtown is already thick with venues. Where there are tourists, there are always possibilities, but I decided to go a different direction. There are a couple different types of restaurant, and I don't mean cuisine categories; First, there are restaurants you go to as an \"occasion\", for a birthday or special night out or that sort of thing. For those times when you're willing to spend half an hour or more getting across town just for dinner. Second, there are restaurants you go to because they're convenient or nearby. Doesn't mean they're bad restaurants, it just means sometimes you want to get something to eat without the hassle of travel, tourists, reservations, etc. This is the sort of place I'd suggest for the areas we've identified. A decent local place for the people who live in the area. Maybe just a sandwich shop or bodega, maybe a general \"American\" restaurant. Pizza places and donut shops already seem well-represented in the target areas, perhaps give the locals a convenient nearby dining option that isn't another pizza shop or Dunkin'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada67a1b",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Conclusion</span><a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef10d2",
   "metadata": {},
   "source": [
    "The intent of this datawork has been to identify candidate areas for a new restaurant in Boston. Using the data available, we settled on using a metric of population per venue as a guide to locating these candidate areas. We've identified several areas with high population, low venue options, and rapid transit access. A stakeholder would need to then take these candidate areas and do additional investigation (eg. zoning requirements, real estate cost and availability, etc.), however I believe we've created a good jumping-off point for further investigation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
